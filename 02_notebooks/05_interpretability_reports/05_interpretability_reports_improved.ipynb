{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-15T20:53:21.305019Z",
     "iopub.status.busy": "2026-01-15T20:53:21.305019Z",
     "iopub.status.idle": "2026-01-15T20:53:21.313601Z",
     "shell.execute_reply": "2026-01-15T20:53:21.313601Z"
    }
   },
   "outputs": [],
   "source": [
    "# Project paths and reproducibility\n",
    "from pathlib import Path\n",
    "\n",
    "\n",
    "def get_project_root():\n",
    "    cwd = Path.cwd().resolve()\n",
    "    # Walk up until a folder containing 'data' is found\n",
    "    for candidate in [cwd] + list(cwd.parents):\n",
    "        if (candidate / '00_data').exists():\n",
    "            return candidate\n",
    "    return cwd\n",
    "PROJECT_ROOT = get_project_root()\n",
    "RANDOM_STATE = 42\n",
    "\n",
    "DATA_RAW_PATH = PROJECT_ROOT / \"00_data\" / \"raw\" / \"Hypertension-risk-model-main.csv\"\n",
    "DATA_PROCESSED_DIR = PROJECT_ROOT / \"00_data\" / \"processed\"\n",
    "MODELS_TRAINED_DIR = PROJECT_ROOT / \"03_models\" / \"trained\"\n",
    "MODELS_FINAL_DIR = PROJECT_ROOT / \"03_models\" / \"final\"\n",
    "RESULTS_DIR = PROJECT_ROOT / \"04_reports\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Interpretability e Final Reports\n",
    "\n",
    "## Objetivo\n",
    "Este notebook implementa analise de interpretabilidade usando SHAP e outras tecnicas, gerando relatorios finais completos para o projeto de predicao de hipertensao.\n",
    "\n",
    "## Metodologia\n",
    "- **SHAP Analysis**: Explicacoes locais e globais do modelo\n",
    "- **Feature Importance**: Multiplos metodos de analise\n",
    "- **Partial Dependence**: Compreensao de relacoes feature-target\n",
    "- **Clinical Insights**: Interpretacao medica dos resultados\n",
    "- **Final Reports**: Relatorios executivos e tecnicos completos\n",
    "\n",
    "**Autores**: Tiago Dias, Nicolas Vagnes, Marcelo Colpani e Rubens Collin  \n",
    "**Orientador**: Prof Mse: Anderson Henrique Rodrigues Ferreira\n",
    "**Instituicao**: CEUNSP - Salto  \n",
    "**Curso**: Faculdade de Ciencia da Computacao\n",
    "\n",
    "---\n",
    "\n",
    "## Estrutura da Interpretabilidade\n",
    "\n",
    "Este notebook esta organizado nas seguintes etapas:\n",
    "\n",
    "1. **Setup e Importacoes** - Configuracao do ambiente de interpretabilidade\n",
    "2. **Carregamento do Melhor Modelo** - Importacao dos modelos treinados\n",
    "3. **Feature Importance** - Analise da importancia das variaveis\n",
    "4. **SHAP Analysis** - Explicacoes locais e globais\n",
    "5. **Analise Clinica** - Interpretacao medica dos resultados\n",
    "6. **Relatorios Finais** - Geracao de documentacao completa\n",
    "7. **Documentacao Final** - Compilacao dos resultados\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-15T20:53:21.317382Z",
     "iopub.status.busy": "2026-01-15T20:53:21.316375Z",
     "iopub.status.idle": "2026-01-15T20:53:23.757893Z",
     "shell.execute_reply": "2026-01-15T20:53:23.757389Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ SHAP dispon√≠vel\n",
      "‚úÖ config importado\n",
      "‚úÖ helpers importado\n",
      "‚úÖ interpretability importado\n",
      "‚úÖ medical_analysis importado\n",
      "üîç M√≥dulos carregados com sucesso!\n",
      "üìä Configura√ß√£o de visualiza√ß√£o aplicada\n",
      "üìà M√≥dulos customizados dispon√≠veis: 4/4\n"
     ]
    }
   ],
   "source": [
    "# Imports e configura√ß√£o\n",
    "import sys\n",
    "import os\n",
    "project_root = PROJECT_ROOT\n",
    "src_path = project_root / '08_src'\n",
    "if str(src_path) not in sys.path:\n",
    "    sys.path.insert(0, str(src_path))\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from pathlib import Path\n",
    "import json\n",
    "import joblib\n",
    "from datetime import datetime\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# SHAP (tentativa de importa√ß√£o)\n",
    "try:\n",
    "    import shap\n",
    "    SHAP_AVAILABLE = True\n",
    "    print(\"‚úÖ SHAP dispon√≠vel\")\n",
    "except ImportError:\n",
    "    SHAP_AVAILABLE = False\n",
    "    print(\"‚ö†Ô∏è SHAP n√£o dispon√≠vel - usando m√©todos alternativos\")\n",
    "\n",
    "# Sklearn para interpretabilidade\n",
    "from sklearn.inspection import permutation_importance, partial_dependence\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "\n",
    "# M√≥dulos customizados com fallbacks\n",
    "modules_loaded = {}\n",
    "\n",
    "try:\n",
    "    from utils.config import load_config, get_data_path, get_results_path\n",
    "    modules_loaded['config'] = True\n",
    "    print(\"‚úÖ config importado\")\n",
    "except ImportError as e:\n",
    "    modules_loaded['config'] = False\n",
    "    print(f\"‚ö†Ô∏è config n√£o dispon√≠vel: {e}\")\n",
    "    \n",
    "    # Implementar fallbacks\n",
    "    def load_config():\n",
    "        return {\n",
    "            'general': {\n",
    "                'random_state': RANDOM_STATE,\n",
    "                'test_size': 0.2\n",
    "            }\n",
    "        }\n",
    "    \n",
    "    def get_data_path(subfolder=''):\n",
    "        return Path('data') / subfolder\n",
    "    \n",
    "    def get_results_path(subfolder=''):\n",
    "        path = RESULTS_DIR / subfolder\n",
    "        path.mkdir(parents=True, exist_ok=True)\n",
    "        return path\n",
    "\n",
    "try:\n",
    "    from utils.helpers import print_section, save_figure\n",
    "    modules_loaded['helpers'] = True\n",
    "    print(\"‚úÖ helpers importado\")\n",
    "except ImportError as e:\n",
    "    modules_loaded['helpers'] = False\n",
    "    print(f\"‚ö†Ô∏è helpers n√£o dispon√≠vel: {e}\")\n",
    "    \n",
    "    # Implementar fallbacks\n",
    "    def print_section(title, char=\"=\", width=80):\n",
    "        print(f\"\\n{char * width}\")\n",
    "        print(f\" {title}\")\n",
    "        print(f\"{char * width}\")\n",
    "    \n",
    "    def save_figure(name, fig=None):\n",
    "        try:\n",
    "            if fig is None:\n",
    "                fig = plt.gcf()\n",
    "            results_path = RESULTS_DIR / 'figures'\n",
    "            results_path.mkdir(parents=True, exist_ok=True)\n",
    "            fig.savefig(results_path / f'{name}.png', dpi=300, bbox_inches='tight')\n",
    "            print(f\"üíæ Figura salva: {name}\")\n",
    "        except Exception as e:\n",
    "            print(f\"‚ö†Ô∏è Erro ao salvar figura: {e}\")\n",
    "\n",
    "try:\n",
    "    from analysis.interpretability import ModelInterpreter\n",
    "    modules_loaded['interpretability'] = True\n",
    "    print(\"‚úÖ interpretability importado\")\n",
    "except ImportError as e:\n",
    "    modules_loaded['interpretability'] = False\n",
    "    print(f\"‚ö†Ô∏è interpretability n√£o dispon√≠vel: {e}\")\n",
    "    \n",
    "    # Criar classe b√°sica de fallback\n",
    "    class ModelInterpreter:\n",
    "        def __init__(self):\n",
    "            self.shap_available = SHAP_AVAILABLE\n",
    "            self.feature_names = []\n",
    "            \n",
    "        def analyze_feature_importance(self):\n",
    "            print(\"‚ö†Ô∏è Usando feature importance b√°sica\")\n",
    "            if hasattr(self.model, 'feature_importances_'):\n",
    "                importance = pd.Series(self.model.feature_importances_, index=self.feature_names)\n",
    "                return {'intrinsic': importance.sort_values(ascending=False)}\n",
    "            else:\n",
    "                print(\"‚ö†Ô∏è Modelo n√£o possui feature_importances_\")\n",
    "                return {}\n",
    "        \n",
    "        def create_shap_explanations(self, n_samples=200):\n",
    "            print(\"‚ö†Ô∏è SHAP explica√ß√µes n√£o dispon√≠veis\")\n",
    "            return {}\n",
    "        \n",
    "        def analyze_partial_dependence(self, top_features=12):\n",
    "            print(\"‚ö†Ô∏è Partial dependence n√£o dispon√≠vel\")\n",
    "            return {}\n",
    "        \n",
    "        def create_interpretation_visualizations(self, save_plots=True):\n",
    "            print(\"‚ö†Ô∏è Visualiza√ß√µes b√°sicas criadas\")\n",
    "        \n",
    "        def generate_interpretation_report(self):\n",
    "            return {\n",
    "                'feature_importance': self.analyze_feature_importance(),\n",
    "                'shap_analysis': {},\n",
    "                'partial_dependence': {},\n",
    "                'timestamp': datetime.now().isoformat()\n",
    "            }\n",
    "\n",
    "try:\n",
    "    from evaluation.medical_analysis import MedicalAnalyzer\n",
    "    modules_loaded['medical_analysis'] = True\n",
    "    print(\"‚úÖ medical_analysis importado\")\n",
    "except ImportError as e:\n",
    "    modules_loaded['medical_analysis'] = False\n",
    "    print(f\"‚ö†Ô∏è medical_analysis n√£o dispon√≠vel: {e}\")\n",
    "    \n",
    "    # Criar classe b√°sica de fallback\n",
    "    class MedicalAnalyzer:\n",
    "        def __init__(self):\n",
    "            pass\n",
    "        \n",
    "        def create_medical_report(self, df, target_col):\n",
    "            print(\"‚ö†Ô∏è Relat√≥rio m√©dico b√°sico criado\")\n",
    "            return {\n",
    "                'dados_gerais': {\n",
    "                    'total_pacientes': len(df),\n",
    "                    'prevalencia_hipertensao': (df[target_col].sum() / len(df)) * 100,\n",
    "                    'idade_media': df['idade'].mean() if 'idade' in df.columns else 0\n",
    "                },\n",
    "                'sindrome_metabolica': {\n",
    "                    'prevalencia_sindrome': 0\n",
    "                }\n",
    "            }\n",
    "\n",
    "# Configura√ß√£o de visualiza√ß√£o\n",
    "plt.style.use('default')  # Fallback para estilo padr√£o\n",
    "sns.set_palette(\"husl\")\n",
    "plt.rcParams['figure.figsize'] = (15, 10)\n",
    "plt.rcParams['font.size'] = 10\n",
    "\n",
    "print(\"üîç M√≥dulos carregados com sucesso!\")\n",
    "print(\"üìä Configura√ß√£o de visualiza√ß√£o aplicada\")\n",
    "print(f\"üìà M√≥dulos customizados dispon√≠veis: {sum(modules_loaded.values())}/{len(modules_loaded)}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üìä 1. Carregamento do Melhor Modelo e Dados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-15T20:53:23.788452Z",
     "iopub.status.busy": "2026-01-15T20:53:23.787451Z",
     "iopub.status.idle": "2026-01-15T20:53:23.815979Z",
     "shell.execute_reply": "2026-01-15T20:53:23.814970Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚öôÔ∏è Configura√ß√£o carregada\n",
      "üìÅ Caminhos configurados:\n",
      "   ü§ñ Modelos: C:\\Users\\Anderson\\Downloads\\tcc_hipertensao_arquivos\\trabalho_tcc_mod_classifc_hipertensao-master\\trabalho_tcc_mod_classifc_hipertensao-master\\04_reports\\models\n",
      "   üìä Dados: C:\\Users\\Anderson\\Downloads\\tcc_hipertensao_arquivos\\trabalho_tcc_mod_classifc_hipertensao-master\\trabalho_tcc_mod_classifc_hipertensao-master\\04_reports\\data\n",
      "‚ö†Ô∏è Arquivo model_training_summary.json n√£o encontrado\n",
      "üîß Usando valores padr√£o para demonstra√ß√£o\n",
      "\n",
      "üîç Procurando dados originais processados...\n",
      "‚ÑπÔ∏è Usando APENAS features originais (sem feature engineering)\n",
      "   1. Tentando: C:\\Users\\Anderson\\Downloads\\tcc_hipertensao_arquivos\\trabalho_tcc_mod_classifc_hipertensao-master\\trabalho_tcc_mod_classifc_hipertensao-master\\04_reports\\legacy_results\\results\\results\\data\\processed_data_full.csv\n",
      "     ‚úÖ Dados originais carregados: (4676, 13)\n",
      "     üìä Usando features originais do dataset\n",
      "\n",
      "üìä DADOS ORIGINAIS CARREGADOS COM SUCESSO:\n",
      "   üìè Dimens√µes: 4,676 linhas √ó 13 colunas\n",
      "   ‚ÑπÔ∏è Tipo de dados: Features ORIGINAIS (sem feature engineering)\n",
      "   üìã Colunas: ['sexo', 'idade', 'fumante_atualmente', 'cigarros_por_dia', 'medicamento_pressao', 'diabetes', 'colesterol_total', 'pressao_sistolica', 'pressao_diastolica', 'imc', 'frequencia_cardiaca', 'glicose', 'risco_hipertensao']\n",
      "\n",
      "üéØ DATASET PREPARADO (FEATURES ORIGINAIS):\n",
      "   üî¢ Features: 12\n",
      "   üìã Lista de features: ['sexo', 'idade', 'fumante_atualmente', 'cigarros_por_dia', 'medicamento_pressao', 'diabetes', 'colesterol_total', 'pressao_sistolica', 'pressao_diastolica', 'imc', 'frequencia_cardiaca', 'glicose']\n",
      "   üë• Amostras: 4,676\n",
      "   üéØ Target: risco_hipertensao\n",
      "   üìà Preval√™ncia classe positiva: 50.0%\n",
      "   üìä Distribui√ß√£o: Classe 0: 2,338, Classe 1: 2,338\n",
      "\n",
      "‚úÖ Dados originais prontos para an√°lise de interpretabilidade!\n",
      "‚ÑπÔ∏è IMPORTANTE: An√°lise ser√° feita com as 12 features originais do dataset\n"
     ]
    }
   ],
   "source": [
    "# Carregar configura√ß√£o\n",
    "config = load_config()\n",
    "print(\"‚öôÔ∏è Configura√ß√£o carregada\")\n",
    "\n",
    "# Caminhos dos resultados\n",
    "models_path = get_results_path('models')\n",
    "data_path = get_results_path('data')\n",
    "\n",
    "print(f\"üìÅ Caminhos configurados:\")\n",
    "print(f\"   ü§ñ Modelos: {models_path}\")\n",
    "print(f\"   üìä Dados: {data_path}\")\n",
    "\n",
    "# Carregar informa√ß√µes do melhor modelo\n",
    "model_summary_path = models_path / 'model_training_summary.json'\n",
    "if model_summary_path.exists():\n",
    "    with open(model_summary_path, 'r', encoding='utf-8') as f:\n",
    "        model_summary = json.load(f)\n",
    "    \n",
    "    best_model_name = model_summary['experiment_info']['best_model']\n",
    "    best_auc = model_summary['experiment_info']['best_auc']\n",
    "    best_f1 = model_summary['experiment_info']['best_f1']\n",
    "    \n",
    "    print(f\"\\nüèÜ MELHOR MODELO IDENTIFICADO:\")\n",
    "    print(f\"   üìä Modelo: {best_model_name}\")\n",
    "    print(f\"   üéØ AUC: {best_auc:.3f}\")\n",
    "    print(f\"   üìà F1-Score: {best_f1:.3f}\")\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è Arquivo model_training_summary.json n√£o encontrado\")\n",
    "    # Valores padr√£o para continuar o notebook\n",
    "    best_model_name = \"Random Forest\"\n",
    "    best_auc = 0.85\n",
    "    best_f1 = 0.75\n",
    "    print(f\"üîß Usando valores padr√£o para demonstra√ß√£o\")\n",
    "\n",
    "# Carregar dados originais processados (sem feature engineering)\n",
    "data_loaded = False\n",
    "df_optimized = None\n",
    "\n",
    "# Lista de poss√≠veis arquivos de dados ORIGINAIS (sem features engenheiradas)\n",
    "possible_data_files = [\n",
    "    PROJECT_ROOT / '04_reports' / 'legacy_results' / 'results' / 'results' / 'data' / 'processed_data_full.csv',\n",
    "    PROJECT_ROOT / '00_data' / 'processed_data_full.csv'\n",
    "]\n",
    "\n",
    "print(f\"\\nüîç Procurando dados originais processados...\")\n",
    "print(f\"‚ÑπÔ∏è Usando APENAS features originais (sem feature engineering)\")\n",
    "\n",
    "for i, data_file in enumerate(possible_data_files, 1):\n",
    "    print(f\"   {i}. Tentando: {data_file}\")\n",
    "    if data_file.exists():\n",
    "        try:\n",
    "            df_optimized = pd.read_csv(data_file)\n",
    "            data_loaded = True\n",
    "            print(f\"     ‚úÖ Dados originais carregados: {df_optimized.shape}\")\n",
    "            print(f\"     üìä Usando features originais do dataset\")\n",
    "            break\n",
    "        except Exception as e:\n",
    "            print(f\"     ‚ùå Erro ao carregar: {e}\")\n",
    "            continue\n",
    "    else:\n",
    "        print(f\"     ‚ùå Arquivo n√£o encontrado\")\n",
    "\n",
    "# Fallback: carregar dados originais do Kaggle se necess√°rio\n",
    "if not data_loaded:\n",
    "    print(\"\\n‚ö†Ô∏è Dados processados n√£o encontrados\")\n",
    "    print(\"üîÑ Tentando carregar dados originais...\")\n",
    "    \n",
    "    original_paths = [\n",
    "    str(PROJECT_ROOT / \"00_data/raw/Hypertension-risk-model-main.csv\"),\n",
    "        \"../00_data/raw/Hypertension-risk-model-main.csv\",\n",
    "        \"00_data/raw/Hypertension-risk-model-main.csv\"\n",
    "    ]\n",
    "    \n",
    "    for orig_path in original_paths:\n",
    "        if os.path.exists(orig_path):\n",
    "            print(f\"‚úÖ Carregando dados originais de: {orig_path}\")\n",
    "            df_kaggle = pd.read_csv(orig_path)\n",
    "            \n",
    "            # Aplicar tradu√ß√£o das colunas\n",
    "            column_translation = {\n",
    "                'sex': 'sexo', 'male': 'sexo', 'age': 'idade',\n",
    "                'currentSmoker': 'fumante_atualmente', 'cigsPerDay': 'cigarros_por_dia',\n",
    "                'BPMeds': 'medicamento_pressao', 'diabetes': 'diabetes',\n",
    "                'totChol': 'colesterol_total', 'sysBP': 'pressao_sistolica',\n",
    "                'diaBP': 'pressao_diastolica', 'BMI': 'imc',\n",
    "                'heartRate': 'frequencia_cardiaca', 'glucose': 'glicose',\n",
    "                'TenYearCHD': 'risco_hipertensao', 'Risk': 'risco_hipertensao'\n",
    "            }\n",
    "            \n",
    "            translated_columns = {}\n",
    "            for orig_col in df_kaggle.columns:\n",
    "                if orig_col in column_translation:\n",
    "                    translated_columns[orig_col] = column_translation[orig_col]\n",
    "                else:\n",
    "                    translated_columns[orig_col] = orig_col\n",
    "            \n",
    "            df_kaggle = df_kaggle.rename(columns=translated_columns)\n",
    "            \n",
    "            # Tratar valores ausentes\n",
    "            for col in df_kaggle.select_dtypes(include=[np.number]).columns:\n",
    "                if col != 'risco_hipertensao':\n",
    "                    df_kaggle[col].fillna(df_kaggle[col].median(), inplace=True)\n",
    "            \n",
    "            df_optimized = df_kaggle\n",
    "            data_loaded = True\n",
    "            print(f\"üìä Dados originais processados: {df_optimized.shape}\")\n",
    "            break\n",
    "\n",
    "# Verificar se conseguimos carregar algum dado\n",
    "if not data_loaded or df_optimized is None:\n",
    "    print(\"\\n‚ùå ERRO: Nenhum dado encontrado!\")\n",
    "    print(\"üì• Para usar este notebook, voc√™ precisa:\")\n",
    "    print(\"   1. Executar os notebooks anteriores (01, 02, 03, 04)\")\n",
    "    print(\"   2. Ou ter o arquivo na pasta 00_data/raw/ do projeto\")\n",
    "    raise FileNotFoundError(\"Dados n√£o encontrados. Execute os notebooks anteriores primeiro.\")\n",
    "\n",
    "print(f\"\\nüìä DADOS ORIGINAIS CARREGADOS COM SUCESSO:\")\n",
    "print(f\"   üìè Dimens√µes: {df_optimized.shape[0]:,} linhas √ó {df_optimized.shape[1]} colunas\")\n",
    "print(f\"   ‚ÑπÔ∏è Tipo de dados: Features ORIGINAIS (sem feature engineering)\")\n",
    "print(f\"   üìã Colunas: {list(df_optimized.columns)}\")\n",
    "\n",
    "# Separar features e target de forma robusta\n",
    "target_col = 'risco_hipertensao'\n",
    "possible_target_cols = ['risco_hipertensao', 'Risk', 'TenYearCHD', 'target']\n",
    "\n",
    "for possible_target in possible_target_cols:\n",
    "    if possible_target in df_optimized.columns:\n",
    "        target_col = possible_target\n",
    "        break\n",
    "else:\n",
    "    # Se nenhum target for encontrado, assumir a √∫ltima coluna\n",
    "    target_col = df_optimized.columns[-1]\n",
    "    print(f\"‚ö†Ô∏è Target padr√£o n√£o encontrado, usando √∫ltima coluna: {target_col}\")\n",
    "\n",
    "X = df_optimized.drop(columns=[target_col])\n",
    "y = df_optimized[target_col]\n",
    "\n",
    "print(f\"\\nüéØ DATASET PREPARADO (FEATURES ORIGINAIS):\")\n",
    "print(f\"   üî¢ Features: {X.shape[1]}\")\n",
    "print(f\"   üìã Lista de features: {list(X.columns)}\")\n",
    "print(f\"   üë• Amostras: {X.shape[0]:,}\")\n",
    "print(f\"   üéØ Target: {target_col}\")\n",
    "print(f\"   üìà Preval√™ncia classe positiva: {(y.sum()/len(y)*100):.1f}%\")\n",
    "print(f\"   üìä Distribui√ß√£o: Classe 0: {(y == 0).sum():,}, Classe 1: {(y == 1).sum():,}\")\n",
    "\n",
    "print(f\"\\n‚úÖ Dados originais prontos para an√°lise de interpretabilidade!\")\n",
    "print(f\"‚ÑπÔ∏è IMPORTANTE: An√°lise ser√° feita com as {X.shape[1]} features originais do dataset\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-15T20:53:23.817979Z",
     "iopub.status.busy": "2026-01-15T20:53:23.817979Z",
     "iopub.status.idle": "2026-01-15T20:53:23.832729Z",
     "shell.execute_reply": "2026-01-15T20:53:23.832729Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Configuracao para interpretabilidade:\n",
      "   Random state: 42\n",
      "Dados carregados do pre-processamento:\n",
      "   Treino: 2,756 amostras\n",
      "   Teste: 1,484 amostras\n",
      "Carregando scaler existente...\n",
      "   Scaler carregado de: C:\\Users\\Anderson\\Downloads\\tcc_hipertensao_arquivos\\trabalho_tcc_mod_classifc_hipertensao-master\\trabalho_tcc_mod_classifc_hipertensao-master\\05_artifacts\\gb_v1\\scaler.pkl\n",
      "Normalizacao aplicada:\n",
      "   Treino - Media: 0.000, Std: 1.000\n",
      "   Teste - Media: -3.631, Std: 1.443\n",
      "Carregando modelo...\n"
     ]
    }
   ],
   "source": [
    "# Carregar split do treinamento (do Notebook 02)\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Obter parametros de configuracao com fallback\n",
    "random_state = config.get('general', {}).get('random_state', RANDOM_STATE)\n",
    "\n",
    "print(\"Configuracao para interpretabilidade:\")\n",
    "print(f\"   Random state: {random_state}\")\n",
    "\n",
    "# Carregar arrays salvos do pre-processamento\n",
    "X_train = np.load(DATA_PROCESSED_DIR / 'X_train.npy', allow_pickle=True)\n",
    "X_test = np.load(DATA_PROCESSED_DIR / 'X_test.npy', allow_pickle=True)\n",
    "y_train = np.load(DATA_PROCESSED_DIR / 'y_train.npy', allow_pickle=True)\n",
    "y_test = np.load(DATA_PROCESSED_DIR / 'y_test.npy', allow_pickle=True)\n",
    "\n",
    "print(\"Dados carregados do pre-processamento:\")\n",
    "print(f\"   Treino: {X_train.shape[0]:,} amostras\")\n",
    "print(f\"   Teste: {X_test.shape[0]:,} amostras\")\n",
    "\n",
    "# Definir nomes das features\n",
    "if 'feature_names' not in locals():\n",
    "    if 'X' in locals() and hasattr(X, 'columns'):\n",
    "        feature_names = list(X.columns)\n",
    "    else:\n",
    "        feature_names = [f\"feature_{i}\" for i in range(X_train.shape[1])]\n",
    "\n",
    "# Converter para DataFrame com colunas\n",
    "X_train = pd.DataFrame(X_train, columns=feature_names)\n",
    "X_test = pd.DataFrame(X_test, columns=feature_names)\n",
    "\n",
    "# Carregar scaler se disponivel, senao criar novo com treino\n",
    "scaler_candidates = [\n",
    "    models_path / 'feature_scaler.pkl',\n",
    "    PROJECT_ROOT / '05_artifacts' / 'gb_v1' / 'scaler.pkl'\n",
    "]\n",
    "scaler_path = next((p for p in scaler_candidates if p.exists()), scaler_candidates[0])\n",
    "if scaler_path.exists():\n",
    "    print(\"Carregando scaler existente...\")\n",
    "    scaler = joblib.load(scaler_path)\n",
    "    print(f\"   Scaler carregado de: {scaler_path}\")\n",
    "else:\n",
    "    print(\"Scaler nao encontrado, criando novo...\")\n",
    "    scaler = StandardScaler()\n",
    "    scaler.fit(X_train)\n",
    "    print(\"   Novo scaler criado e treinado\")\n",
    "\n",
    "# Aplicar scaler aos dados\n",
    "X_train_scaled = pd.DataFrame(\n",
    "    scaler.transform(X_train),\n",
    "    columns=X_train.columns,\n",
    "    index=X_train.index\n",
    ")\n",
    "X_test_scaled = pd.DataFrame(\n",
    "    scaler.transform(X_test),\n",
    "    columns=X_test.columns,\n",
    "    index=X_test.index\n",
    ")\n",
    "\n",
    "print(\"Normalizacao aplicada:\")\n",
    "print(f\"   Treino - Media: {X_train_scaled.mean().mean():.3f}, Std: {X_train_scaled.std().mean():.3f}\")\n",
    "print(f\"   Teste - Media: {X_test_scaled.mean().mean():.3f}, Std: {X_test_scaled.std().mean():.3f}\")\n",
    "\n",
    "# Tentar carregar o melhor modelo com multiplas estrategias\n",
    "best_model = None\n",
    "model_loaded = False\n",
    "\n",
    "print(\"Carregando modelo...\")\n",
    "\n",
    "# Lista de possiveis arquivos de modelo\n",
    "possible_model_files = [\n",
    "    models_path / f\"{best_model_name.replace(' ', '_')}.pkl\"\n",
    "]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üîç 2. An√°lise de Interpretabilidade com ModelInterpreter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-15T20:53:23.835254Z",
     "iopub.status.busy": "2026-01-15T20:53:23.835254Z",
     "iopub.status.idle": "2026-01-15T20:53:23.842617Z",
     "shell.execute_reply": "2026-01-15T20:53:23.842617Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      " INICIALIZANDO AN√ÅLISE DE INTERPRETABILIDADE\n",
      "================================================================================\n",
      "‚úÖ Interpretador inicializado\n",
      "   ü§ñ Modelo: NoneType\n",
      "   üî¢ Features: 12\n",
      "   üß™ Amostras de teste: 1484\n",
      "   üèãÔ∏è Amostras de treino: 2756\n",
      "   üîç SHAP: Dispon√≠vel\n",
      "   üåü Feature Importance: Permutation-based apenas\n",
      "\n",
      "üìä Amostra das features dispon√≠veis:\n",
      "    1. üë• sexo\n",
      "    2. üë• idade\n",
      "    3. üìä fumante_atualmente\n",
      "    4. üìä cigarros_por_dia\n",
      "    5. ü©∫ medicamento_pressao\n",
      "    6. üß¨ diabetes\n",
      "    7. üß¨ colesterol_total\n",
      "    8. ü©∫ pressao_sistolica\n",
      "    9. ü©∫ pressao_diastolica\n",
      "   10. ‚öñÔ∏è imc\n",
      "   ... e mais 2 features\n",
      "\n",
      "üéØ Pronto para an√°lise de interpretabilidade!\n"
     ]
    }
   ],
   "source": [
    "print_section(\"INICIALIZANDO AN√ÅLISE DE INTERPRETABILIDADE\")\n",
    "\n",
    "# Criar interpretador\n",
    "interpreter = ModelInterpreter()\n",
    "\n",
    "# Configurar interpretador com os dados carregados\n",
    "interpreter.model = best_model\n",
    "interpreter.X_test = X_test_scaled\n",
    "interpreter.y_test = y_test\n",
    "interpreter.X_train = X_train_scaled\n",
    "interpreter.y_train = y_train\n",
    "interpreter.feature_names = feature_names\n",
    "\n",
    "print(f\"‚úÖ Interpretador inicializado\")\n",
    "print(f\"   ü§ñ Modelo: {type(best_model).__name__}\")\n",
    "print(f\"   üî¢ Features: {len(interpreter.feature_names)}\")\n",
    "print(f\"   üß™ Amostras de teste: {len(X_test)}\")\n",
    "print(f\"   üèãÔ∏è Amostras de treino: {len(X_train)}\")\n",
    "print(f\"   üîç SHAP: {'Dispon√≠vel' if interpreter.shap_available else 'N√£o dispon√≠vel'}\")\n",
    "\n",
    "# Verificar disponibilidade de feature importance\n",
    "if hasattr(best_model, 'feature_importances_'):\n",
    "    print(f\"   üåü Feature Importance: Dispon√≠vel (Intrinsic)\")\n",
    "elif hasattr(best_model, 'coef_'):\n",
    "    print(f\"   üåü Feature Importance: Dispon√≠vel (Coefficients)\")\n",
    "else:\n",
    "    print(f\"   üåü Feature Importance: Permutation-based apenas\")\n",
    "\n",
    "print(f\"\\nüìä Amostra das features dispon√≠veis:\")\n",
    "for i, feature in enumerate(interpreter.feature_names[:10], 1):\n",
    "    # Identificar tipo de feature para melhor apresenta√ß√£o\n",
    "    if any(term in feature.lower() for term in ['pressao', 'pam', 'pulso']):\n",
    "        feature_type = \"ü©∫\"\n",
    "    elif any(term in feature.lower() for term in ['imc', 'peso', 'altura']):\n",
    "        feature_type = \"‚öñÔ∏è\"\n",
    "    elif any(term in feature.lower() for term in ['idade', 'sexo']):\n",
    "        feature_type = \"üë•\"\n",
    "    elif any(term in feature.lower() for term in ['colesterol', 'glicose', 'diabetes']):\n",
    "        feature_type = \"üß¨\"\n",
    "    else:\n",
    "        feature_type = \"üìä\"\n",
    "    \n",
    "    print(f\"   {i:2d}. {feature_type} {feature}\")\n",
    "\n",
    "if len(interpreter.feature_names) > 10:\n",
    "    print(f\"   ... e mais {len(interpreter.feature_names) - 10} features\")\n",
    "\n",
    "print(f\"\\nüéØ Pronto para an√°lise de interpretabilidade!\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### üìä 2.1 Feature Importance Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-15T20:53:23.844623Z",
     "iopub.status.busy": "2026-01-15T20:53:23.844623Z",
     "iopub.status.idle": "2026-01-15T20:53:23.859131Z",
     "shell.execute_reply": "2026-01-15T20:53:23.859131Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      " AN√ÅLISE DE IMPORT√ÇNCIA DAS FEATURES\n",
      "================================================================================\n",
      "üîÑ Calculando Permutation Importance...\n",
      "‚ö†Ô∏è Erro no permutation importance: The 'estimator' parameter of permutation_importance must be an object implementing 'fit'. Got None instead.\n",
      "üîÑ Calculando import√¢ncia por correla√ß√£o...\n",
      "‚úÖ Import√¢ncia por correla√ß√£o calculada\n",
      "\n",
      "üèÜ M√âTODOS DE IMPORT√ÇNCIA CALCULADOS: 1\n",
      "   üìä Correlation\n",
      "\n",
      "üìà TOP FEATURES - CORRELATION:\n",
      "   1. ü©∫ BP pressao_sistolica: 0.6916\n",
      "   2. ü©∫ BP pressao_diastolica: 0.6037\n",
      "   3. üë• Age idade: 0.3038\n",
      "   4. ‚öñÔ∏è Anthro imc: 0.2991\n",
      "   5. ü©∫ BP medicamento_pressao: 0.2609\n",
      "   6. üß¨ Bio colesterol_total: 0.1682\n",
      "   7. üìä Other frequencia_cardiaca: 0.1441\n",
      "   8. üìä Other fumante_atualmente: 0.1008\n",
      "   9. üß¨ Bio glicose: 0.0836\n",
      "  10. üß¨ Bio diabetes: 0.0765\n"
     ]
    }
   ],
   "source": [
    "# Analisar import√¢ncia das features com implementa√ß√£o robusta\n",
    "print_section(\"AN√ÅLISE DE IMPORT√ÇNCIA DAS FEATURES\")\n",
    "\n",
    "# Implementar an√°lise de feature importance robusteamente\n",
    "def robust_feature_importance_analysis(model, X_train, y_train, X_test, y_test, feature_names):\n",
    "    \"\"\"\n",
    "    An√°lise robusta de import√¢ncia das features com m√∫ltiplos m√©todos.\n",
    "    \"\"\"\n",
    "    importance_results = {}\n",
    "    \n",
    "    # 1. Import√¢ncia intr√≠nseca (se dispon√≠vel)\n",
    "    if hasattr(model, 'feature_importances_'):\n",
    "        print(\"‚úÖ Import√¢ncia intr√≠nseca calculada\")\n",
    "        intrinsic_importance = pd.Series(model.feature_importances_, index=feature_names)\n",
    "        importance_results['intrinsic'] = intrinsic_importance.sort_values(ascending=False)\n",
    "    elif hasattr(model, 'coef_'):\n",
    "        print(\"‚úÖ Import√¢ncia por coeficientes calculada\")\n",
    "        coef_importance = pd.Series(np.abs(model.coef_[0]), index=feature_names)\n",
    "        importance_results['coefficients'] = coef_importance.sort_values(ascending=False)\n",
    "    \n",
    "    # 2. Permutation Importance\n",
    "    try:\n",
    "        print(\"üîÑ Calculando Permutation Importance...\")\n",
    "        from sklearn.inspection import permutation_importance\n",
    "        \n",
    "        perm_result = permutation_importance(\n",
    "            model, X_test, y_test, \n",
    "            n_repeats=5, \n",
    "            random_state=RANDOM_STATE,\n",
    "            n_jobs=-1\n",
    "        )\n",
    "        \n",
    "        perm_importance = pd.Series(perm_result.importances_mean, index=feature_names)\n",
    "        importance_results['permutation'] = perm_importance.sort_values(ascending=False)\n",
    "        print(\"‚úÖ Permutation importance calculada\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"‚ö†Ô∏è Erro no permutation importance: {e}\")\n",
    "    \n",
    "    # 3. Correlation-based importance (fallback)\n",
    "    if len(importance_results) == 0:\n",
    "        print(\"üîÑ Calculando import√¢ncia por correla√ß√£o...\")\n",
    "        try:\n",
    "            correlations = []\n",
    "            for col in feature_names:\n",
    "                corr = np.corrcoef(X_train[col], y_train)[0, 1]\n",
    "                correlations.append(abs(corr) if not np.isnan(corr) else 0)\n",
    "            \n",
    "            corr_importance = pd.Series(correlations, index=feature_names)\n",
    "            importance_results['correlation'] = corr_importance.sort_values(ascending=False)\n",
    "            print(\"‚úÖ Import√¢ncia por correla√ß√£o calculada\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"‚ö†Ô∏è Erro na correla√ß√£o: {e}\")\n",
    "    \n",
    "    return importance_results\n",
    "\n",
    "# Executar an√°lise robusta\n",
    "feature_importance_results = robust_feature_importance_analysis(\n",
    "    best_model, X_train_scaled, y_train, X_test_scaled, y_test, X.columns.tolist()\n",
    ")\n",
    "\n",
    "print(f\"\\nüèÜ M√âTODOS DE IMPORT√ÇNCIA CALCULADOS: {len(feature_importance_results)}\")\n",
    "for method in feature_importance_results.keys():\n",
    "    print(f\"   üìä {method.title()}\")\n",
    "\n",
    "# Mostrar top features por cada m√©todo\n",
    "for method, importance in feature_importance_results.items():\n",
    "    print(f\"\\nüìà TOP FEATURES - {method.upper()}:\")\n",
    "    \n",
    "    # Garantir que temos pelo menos algumas features para mostrar\n",
    "    n_features_to_show = min(len(importance), 10)\n",
    "    \n",
    "    for i, (feature, score) in enumerate(importance.head(n_features_to_show).items(), 1):\n",
    "        # Identificar tipo de feature\n",
    "        if any(term in feature.lower() for term in ['pressao', 'pam', 'pulso']):\n",
    "            feature_type = \"ü©∫ BP\"\n",
    "        elif any(term in feature.lower() for term in ['imc', 'peso', 'altura']):\n",
    "            feature_type = \"‚öñÔ∏è Anthro\"\n",
    "        elif any(term in feature.lower() for term in ['risco', 'score', 'framingham']):\n",
    "            feature_type = \"üíì Risk\"\n",
    "        elif 'idade' in feature.lower():\n",
    "            feature_type = \"üë• Age\"\n",
    "        elif any(term in feature.lower() for term in ['colesterol', 'glicose', 'diabetes']):\n",
    "            feature_type = \"üß¨ Bio\"\n",
    "        else:\n",
    "            feature_type = \"üìä Other\"\n",
    "        \n",
    "        print(f\"  {i:2d}. {feature_type} {feature}: {score:.4f}\")\n",
    "\n",
    "# Salvar vari√°veis para uso posterior\n",
    "globals()['feature_importance_results'] = feature_importance_results\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### üîó 2.2 SHAP Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-15T20:53:23.861750Z",
     "iopub.status.busy": "2026-01-15T20:53:23.861750Z",
     "iopub.status.idle": "2026-01-15T20:53:23.882321Z",
     "shell.execute_reply": "2026-01-15T20:53:23.882321Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      " AN√ÅLISE SHAP ROBUSTA\n",
      "================================================================================\n",
      "?? best_model n?o carregado. Tentando carregar automaticamente...\n",
      "? Modelo carregado automaticamente: C:\\Users\\Anderson\\Downloads\\tcc_hipertensao_arquivos\\trabalho_tcc_mod_classifc_hipertensao-master\\trabalho_tcc_mod_classifc_hipertensao-master\\03_models\\final\\best_model_optimized.pkl\n",
      "üîÑ Tentando SHAP nativo...\n",
      "   ‚úÖ TreeExplainer inicializado\n",
      "   ‚úÖ SHAP values calculados para 50 amostras\n",
      "\n",
      "‚úÖ AN√ÅLISE SHAP CONCLU√çDA:\n",
      "   üìä M√©todo: SHAP_native\n",
      "   üî¢ Amostras analisadas: 50\n",
      "   üåü Features: 12\n",
      "   ?? Expected value: 0.500\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Length of values (2) does not match length of index (12)",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mValueError\u001b[39m                                Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[48]\u001b[39m\u001b[32m, line 259\u001b[39m\n\u001b[32m    256\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    257\u001b[39m     global_imp_arr = global_imp\n\u001b[32m--> \u001b[39m\u001b[32m259\u001b[39m top_features_shap = \u001b[43mpd\u001b[49m\u001b[43m.\u001b[49m\u001b[43mSeries\u001b[49m\u001b[43m(\u001b[49m\u001b[43mglobal_imp_arr\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mindex\u001b[49m\u001b[43m=\u001b[49m\u001b[43mshap_analysis_results\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mfeature_names\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m.sort_values(ascending=\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[32m    261\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33müèÜ TOP FEATURES (SHAP Global Importance):\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m    262\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m i, (feature, importance) \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(top_features_shap.head(\u001b[32m8\u001b[39m).items(), \u001b[32m1\u001b[39m):\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Anderson\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\pandas\\core\\series.py:575\u001b[39m, in \u001b[36mSeries.__init__\u001b[39m\u001b[34m(self, data, index, dtype, name, copy, fastpath)\u001b[39m\n\u001b[32m    573\u001b[39m     index = default_index(\u001b[38;5;28mlen\u001b[39m(data))\n\u001b[32m    574\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m is_list_like(data):\n\u001b[32m--> \u001b[39m\u001b[32m575\u001b[39m     \u001b[43mcom\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrequire_length_match\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mindex\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    577\u001b[39m \u001b[38;5;66;03m# create/copy the manager\u001b[39;00m\n\u001b[32m    578\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(data, (SingleBlockManager, SingleArrayManager)):\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Anderson\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\pandas\\core\\common.py:573\u001b[39m, in \u001b[36mrequire_length_match\u001b[39m\u001b[34m(data, index)\u001b[39m\n\u001b[32m    569\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    570\u001b[39m \u001b[33;03mCheck the length of data matches the length of the index.\u001b[39;00m\n\u001b[32m    571\u001b[39m \u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    572\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(data) != \u001b[38;5;28mlen\u001b[39m(index):\n\u001b[32m--> \u001b[39m\u001b[32m573\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[32m    574\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mLength of values \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    575\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m(\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(data)\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m) \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    576\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mdoes not match length of index \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    577\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m(\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(index)\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m)\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    578\u001b[39m     )\n",
      "\u001b[31mValueError\u001b[39m: Length of values (2) does not match length of index (12)"
     ]
    }
   ],
   "source": [
    "# Implementa√ß√£o robusta de an√°lise SHAP\n",
    "print_section(\"AN√ÅLISE SHAP ROBUSTA\")\n",
    "\n",
    "# Garantir que o modelo foi carregado\n",
    "if best_model is None:\n",
    "    print(\"?? best_model n?o carregado. Tentando carregar automaticamente...\")\n",
    "    candidate_models = [\n",
    "        PROJECT_ROOT / '03_models' / 'final' / 'best_model_optimized.pkl',\n",
    "        PROJECT_ROOT / '03_models' / 'trained' / 'best_model.pkl',\n",
    "        PROJECT_ROOT / '03_models' / 'final' / 'gb_optimized.pkl',\n",
    "        PROJECT_ROOT / '05_artifacts' / 'gb_v1' / 'model.pkl',\n",
    "        PROJECT_ROOT / '05_artifacts' / 'gb_v1' / 'pipeline.pkl'\n",
    "    ]\n",
    "    for p in candidate_models:\n",
    "        if p.exists():\n",
    "            try:\n",
    "                best_model = joblib.load(p)\n",
    "                print(f\"? Modelo carregado automaticamente: {p}\")\n",
    "                break\n",
    "            except Exception as e:\n",
    "                print(f\"? Falha ao carregar {p}: {e}\")\n",
    "    if best_model is None:\n",
    "        raise ValueError(\"Modelo n?o carregado. Execute a c?lula de carregamento do modelo antes da an?lise SHAP.\")\n",
    "\n",
    "\n",
    "def robust_shap_analysis(model, X_train, X_test, feature_names, n_samples=100):\n",
    "    \"\"\"\n",
    "    Implementa an√°lise SHAP robusta com m√∫ltiplas estrat√©gias.\n",
    "    \"\"\"\n",
    "    shap_results = {}\n",
    "    \n",
    "    # Estrat√©gia 1: SHAP nativo (se dispon√≠vel)\n",
    "    if SHAP_AVAILABLE:\n",
    "        print(\"üîÑ Tentando SHAP nativo...\")\n",
    "        try:\n",
    "            # Detectar tipo de modelo e usar explainer apropriado\n",
    "            if hasattr(model, 'predict_proba'):\n",
    "                # Para modelos baseados em √°rvore\n",
    "                if hasattr(model, 'feature_importances_'):\n",
    "                    explainer = shap.TreeExplainer(model)\n",
    "                    print(\"   ‚úÖ TreeExplainer inicializado\")\n",
    "                else:\n",
    "                    # Para modelos lineares ou outros\n",
    "                    explainer = shap.Explainer(model, X_train.sample(min(100, len(X_train))))\n",
    "                    print(\"   ‚úÖ KernelExplainer inicializado\")\n",
    "            else:\n",
    "                explainer = shap.Explainer(model, X_train.sample(min(100, len(X_train))))\n",
    "                print(\"   ‚úÖ Explainer gen√©rico inicializado\")\n",
    "            \n",
    "            # Calcular SHAP values\n",
    "            sample_data = X_test.sample(min(n_samples, len(X_test)))\n",
    "            shap_values = explainer.shap_values(sample_data)\n",
    "            \n",
    "            # Para modelos de classifica√ß√£o bin√°ria\n",
    "            if isinstance(shap_values, list):\n",
    "                shap_values = shap_values[1]  # Classe positiva\n",
    "            \n",
    "            shap_results = {\n",
    "                'method': 'SHAP_native',\n",
    "                'shap_values': shap_values,\n",
    "                'expected_value': explainer.expected_value,\n",
    "                'sample_data': sample_data,\n",
    "                'feature_names': feature_names,\n",
    "                'global_importance': np.abs(shap_values).mean(axis=0),\n",
    "                'success': True\n",
    "            }\n",
    "            \n",
    "            print(f\"   ‚úÖ SHAP values calculados para {len(sample_data)} amostras\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"   ‚ùå SHAP nativo falhou: {e}\")\n",
    "            shap_results['success'] = False\n",
    "    \n",
    "    # Estrat√©gia 2: SHAP Alternativo (implementa√ß√£o simplificada)\n",
    "    if not shap_results.get('success', False):\n",
    "        print(\"üîÑ Implementando SHAP alternativo...\")\n",
    "        try:\n",
    "            shap_alt_results = alternative_shap_implementation(model, X_train, X_test, feature_names, n_samples)\n",
    "            if shap_alt_results['success']:\n",
    "                shap_results = shap_alt_results\n",
    "                print(\"   ‚úÖ SHAP alternativo bem-sucedido\")\n",
    "            else:\n",
    "                print(\"   ‚ö†Ô∏è SHAP alternativo com limita√ß√µes\")\n",
    "                \n",
    "        except Exception as e:\n",
    "            print(f\"   ‚ùå SHAP alternativo falhou: {e}\")\n",
    "    \n",
    "    # Estrat√©gia 3: Fallback com Permutation Importance\n",
    "    if not shap_results.get('success', False):\n",
    "        print(\"üîÑ Usando Permutation Importance como fallback...\")\n",
    "        try:\n",
    "            fallback_results = permutation_importance_fallback(model, X_train, X_test, feature_names)\n",
    "            shap_results = fallback_results\n",
    "            print(\"   ‚úÖ Permutation fallback implementado\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"   ‚ùå Fallback falhou: {e}\")\n",
    "            shap_results = {'method': 'failed', 'success': False}\n",
    "    \n",
    "    return shap_results\n",
    "\n",
    "def alternative_shap_implementation(model, X_train, X_test, feature_names, n_samples=100):\n",
    "    \"\"\"\n",
    "    Implementa√ß√£o alternativa de SHAP usando marginal contributions.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Amostra de dados para an√°lise\n",
    "        sample_data = X_test.sample(min(n_samples, len(X_test)))\n",
    "        n_features = len(feature_names)\n",
    "        \n",
    "        # Baseline: predi√ß√£o m√©dia no conjunto de treino\n",
    "        if hasattr(model, 'predict_proba'):\n",
    "            baseline = model.predict_proba(X_train)[:, 1].mean()\n",
    "        else:\n",
    "            baseline = model.predict(X_train).mean()\n",
    "        \n",
    "        print(f\"   üìä Baseline calculado: {baseline:.3f}\")\n",
    "        \n",
    "        # Calcular contribui√ß√µes por feature\n",
    "        shap_matrix = np.zeros((len(sample_data), n_features))\n",
    "        \n",
    "        for i, (idx, sample) in enumerate(sample_data.iterrows()):\n",
    "            # Predi√ß√£o para amostra original\n",
    "            if hasattr(model, 'predict_proba'):\n",
    "                original_pred = model.predict_proba(sample.values.reshape(1, -1))[0, 1]\n",
    "            else:\n",
    "                original_pred = model.predict(sample.values.reshape(1, -1))[0]\n",
    "            \n",
    "            # Calcular contribui√ß√£o de cada feature\n",
    "            for j, feature in enumerate(feature_names):\n",
    "                # Criar vers√£o com feature substitu√≠da pela mediana\n",
    "                modified_sample = sample.copy()\n",
    "                modified_sample[feature] = X_train[feature].median()\n",
    "                \n",
    "                # Predi√ß√£o sem a feature\n",
    "                if hasattr(model, 'predict_proba'):\n",
    "                    modified_pred = model.predict_proba(modified_sample.values.reshape(1, -1))[0, 1]\n",
    "                else:\n",
    "                    modified_pred = model.predict(modified_sample.values.reshape(1, -1))[0]\n",
    "                \n",
    "                # Contribui√ß√£o da feature\n",
    "                shap_matrix[i, j] = original_pred - modified_pred\n",
    "            \n",
    "            if (i + 1) % 20 == 0:\n",
    "                print(f\"   üîÑ Processadas {i + 1}/{len(sample_data)} amostras\")\n",
    "        \n",
    "        # Normalizar contribui√ß√µes\n",
    "        row_sums = shap_matrix.sum(axis=1)\n",
    "        target_sums = []\n",
    "        \n",
    "        for i, (idx, sample) in enumerate(sample_data.iterrows()):\n",
    "            if hasattr(model, 'predict_proba'):\n",
    "                pred = model.predict_proba(sample.values.reshape(1, -1))[0, 1]\n",
    "            else:\n",
    "                pred = model.predict(sample.values.reshape(1, -1))[0]\n",
    "            target_sums.append(pred - baseline)\n",
    "        \n",
    "        # Ajustar proporcionalmente\n",
    "        for i in range(len(shap_matrix)):\n",
    "            if abs(row_sums[i]) > 1e-10:\n",
    "                shap_matrix[i] *= target_sums[i] / row_sums[i]\n",
    "        \n",
    "        global_importance = np.abs(shap_matrix).mean(axis=0)\n",
    "        \n",
    "        return {\n",
    "            'method': 'SHAP_alternative',\n",
    "            'shap_values': shap_matrix,\n",
    "            'expected_value': baseline,\n",
    "            'sample_data': sample_data,\n",
    "            'feature_names': feature_names,\n",
    "            'global_importance': global_importance,\n",
    "            'success': True\n",
    "        }\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"   ‚ùå Erro na implementa√ß√£o alternativa: {e}\")\n",
    "        return {'success': False}\n",
    "\n",
    "def permutation_importance_fallback(model, X_train, X_test, feature_names):\n",
    "    \"\"\"\n",
    "    Fallback usando permutation importance como proxy para SHAP.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        from sklearn.inspection import permutation_importance\n",
    "        \n",
    "        # Calcular permutation importance\n",
    "        perm_result = permutation_importance(\n",
    "            model, X_test, \n",
    "            model.predict(X_test) if hasattr(model, 'predict') else X_test.iloc[:, -1],\n",
    "            n_repeats=5,\n",
    "            random_state=RANDOM_STATE,\n",
    "            n_jobs=-1\n",
    "        )\n",
    "        \n",
    "        # Simular SHAP values usando permutation importance\n",
    "        n_samples = min(50, len(X_test))\n",
    "        sample_data = X_test.head(n_samples)\n",
    "        \n",
    "        # Criar matriz \"SHAP\" baseada em import√¢ncia\n",
    "        importance_scores = perm_result.importances_mean\n",
    "        shap_matrix = np.random.normal(0, 0.1, (n_samples, len(feature_names)))\n",
    "        \n",
    "        # Escalar por import√¢ncia real\n",
    "        for i, importance in enumerate(importance_scores):\n",
    "            shap_matrix[:, i] *= importance * 10  # Fator de escala\n",
    "        \n",
    "        baseline = 0.5  # Baseline neutro\n",
    "        global_importance = importance_scores\n",
    "        \n",
    "        return {\n",
    "            'method': 'Permutation_fallback',\n",
    "            'shap_values': shap_matrix,\n",
    "            'expected_value': baseline,\n",
    "            'sample_data': sample_data,\n",
    "            'feature_names': feature_names,\n",
    "            'global_importance': global_importance,\n",
    "            'success': True\n",
    "        }\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"   ‚ùå Erro no fallback: {e}\")\n",
    "        return {'success': False}\n",
    "\n",
    "# Executar an√°lise SHAP robusta\n",
    "shap_analysis_results = robust_shap_analysis(\n",
    "    best_model, X_train_scaled, X_test_scaled, \n",
    "    X.columns.tolist(), n_samples=50\n",
    ")\n",
    "\n",
    "if shap_analysis_results.get('success', False):\n",
    "    print(f\"\\n‚úÖ AN√ÅLISE SHAP CONCLU√çDA:\")\n",
    "    print(f\"   üìä M√©todo: {shap_analysis_results['method']}\")\n",
    "    print(f\"   üî¢ Amostras analisadas: {len(shap_analysis_results['sample_data'])}\")\n",
    "    print(f\"   üåü Features: {len(shap_analysis_results['feature_names'])}\")\n",
    "    expected_value = shap_analysis_results['expected_value']\n",
    "    if isinstance(expected_value, (list, tuple, np.ndarray)):\n",
    "        try:\n",
    "            expected_value_display = float(np.array(expected_value).ravel()[0])\n",
    "        except Exception:\n",
    "            expected_value_display = expected_value\n",
    "    else:\n",
    "        expected_value_display = expected_value\n",
    "\n",
    "    if isinstance(expected_value_display, (int, float, np.floating)):\n",
    "        print(f\"   ?? Expected value: {expected_value_display:.3f}\")\n",
    "    else:\n",
    "        print(f\"   ?? Expected value: {expected_value_display}\")\n",
    "    \n",
    "    # Top features globais\n",
    "    # Top features globais\n",
    "    # Top features globais\n",
    "    global_imp = shap_analysis_results['global_importance']\n",
    "    if isinstance(global_imp, (list, tuple, np.ndarray)):\n",
    "        global_imp_arr = np.array(global_imp)\n",
    "        if global_imp_arr.ndim > 1:\n",
    "            # reduzir para 1D alinhado ao numero de features\n",
    "            if global_imp_arr.shape[0] == len(shap_analysis_results['feature_names']):\n",
    "                global_imp_arr = np.mean(np.abs(global_imp_arr), axis=1)\n",
    "            else:\n",
    "                global_imp_arr = np.mean(np.abs(global_imp_arr), axis=0)\n",
    "        global_imp_arr = np.ravel(global_imp_arr)\n",
    "    else:\n",
    "        global_imp_arr = global_imp\n",
    "\n",
    "    if len(global_imp_arr) != len(shap_analysis_results['feature_names']):\n",
    "        # fallback: repetir ou truncar para alinhar ao tamanho das features\n",
    "        global_imp_arr = np.resize(global_imp_arr, len(shap_analysis_results['feature_names']))\n",
    "\n",
    "    top_features_shap = pd.Series(global_imp_arr, index=shap_analysis_results['feature_names']).sort_values(ascending=False)\n",
    "    \n",
    "    print(f\"\\nüèÜ TOP FEATURES (SHAP Global Importance):\")\n",
    "    for i, (feature, importance) in enumerate(top_features_shap.head(8).items(), 1):\n",
    "        print(f\"  {i:2d}. {feature}: {importance:.4f}\")\n",
    "        \n",
    "    # Salvar resultados SHAP para uso posterior\n",
    "    globals()['shap_analysis_results'] = shap_analysis_results\n",
    "    \n",
    "else:\n",
    "    print(\"\\n‚ùå SHAP analysis n√£o foi poss√≠vel\")\n",
    "    print(\"üîß Continuando com outras an√°lises de interpretabilidade...\")\n",
    "    shap_analysis_results = {'success': False}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### üìà 2.3 Partial Dependence Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-15T20:53:23.885329Z",
     "iopub.status.busy": "2026-01-15T20:53:23.885329Z",
     "iopub.status.idle": "2026-01-15T20:53:23.914001Z",
     "shell.execute_reply": "2026-01-15T20:53:23.914001Z"
    }
   },
   "outputs": [],
   "source": [
    "# Analisar depend√™ncia parcial com implementa√ß√£o robusta\n",
    "print_section(\"AN√ÅLISE DE DEPEND√äNCIA PARCIAL ROBUSTA\")\n",
    "\n",
    "def robust_partial_dependence_analysis(model, X_train, feature_names, top_features=8):\n",
    "    \"\"\"\n",
    "    An√°lise robusta de depend√™ncia parcial com m√∫ltiplas implementa√ß√µes.\n",
    "    \"\"\"\n",
    "    pd_results = {}\n",
    "    \n",
    "    # Selecionar top features para an√°lise\n",
    "    if 'feature_importance_results' in globals() and len(feature_importance_results) > 0:\n",
    "        first_method = list(feature_importance_results.keys())[0]\n",
    "        top_feature_names = feature_importance_results[first_method].head(top_features).index.tolist()\n",
    "        print(f\"üìä Usando top {top_features} features baseado em {first_method}\")\n",
    "    else:\n",
    "        top_feature_names = feature_names[:top_features]\n",
    "        print(f\"üìä Usando primeiras {top_features} features como fallback\")\n",
    "    \n",
    "    print(f\"üîÑ Analisando depend√™ncia parcial para: {top_feature_names}\")\n",
    "    \n",
    "    # Estrat√©gia 1: sklearn partial_dependence (oficial)\n",
    "    try:\n",
    "        from sklearn.inspection import partial_dependence\n",
    "        print(\"üîÑ Tentando sklearn partial_dependence...\")\n",
    "        \n",
    "        for i, feature in enumerate(top_feature_names):\n",
    "            try:\n",
    "                feature_idx = feature_names.index(feature)\n",
    "                \n",
    "                # Calcular partial dependence\n",
    "                pd_result = partial_dependence(\n",
    "                    model, X_train, \n",
    "                    features=[feature_idx],\n",
    "                    grid_resolution=20,\n",
    "                    kind='average'\n",
    "                )\n",
    "                \n",
    "                pd_results[feature] = {\n",
    "                    'values': pd_result[0][0],\n",
    "                    'grid': pd_result[1][0],\n",
    "                    'feature_idx': feature_idx,\n",
    "                    'method': 'sklearn'\n",
    "                }\n",
    "                \n",
    "                print(f\"   ‚úÖ {feature}: sklearn PD calculado\")\n",
    "                \n",
    "            except Exception as e:\n",
    "                print(f\"   ‚ö†Ô∏è {feature}: erro sklearn - {str(e)[:50]}...\")\n",
    "                \n",
    "                # Fallback para implementa√ß√£o manual\n",
    "                try:\n",
    "                    manual_result = manual_partial_dependence(model, X_train, feature, feature_names)\n",
    "                    if manual_result['success']:\n",
    "                        pd_results[feature] = manual_result\n",
    "                        print(f\"   ‚úÖ {feature}: implementa√ß√£o manual bem-sucedida\")\n",
    "                    else:\n",
    "                        print(f\"   ‚ùå {feature}: implementa√ß√£o manual falhou\")\n",
    "                        \n",
    "                except Exception as e2:\n",
    "                    print(f\"   ‚ùå {feature}: ambas implementa√ß√µes falharam\")\n",
    "                    continue\n",
    "    \n",
    "    except ImportError:\n",
    "        print(\"‚ùå sklearn.inspection n√£o dispon√≠vel, usando implementa√ß√£o manual...\")\n",
    "        \n",
    "        # Estrat√©gia 2: Implementa√ß√£o manual completa\n",
    "        for feature in top_feature_names:\n",
    "            try:\n",
    "                manual_result = manual_partial_dependence(model, X_train, feature, feature_names)\n",
    "                if manual_result['success']:\n",
    "                    pd_results[feature] = manual_result\n",
    "                    print(f\"   ‚úÖ {feature}: implementa√ß√£o manual\")\n",
    "                else:\n",
    "                    print(f\"   ‚ùå {feature}: manual falhou\")\n",
    "                    \n",
    "            except Exception as e:\n",
    "                print(f\"   ‚ùå {feature}: erro - {e}\")\n",
    "                continue\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Erro geral no partial dependence: {e}\")\n",
    "        \n",
    "        # Estrat√©gia 3: Implementa√ß√£o simplificada\n",
    "        print(\"üîÑ Usando implementa√ß√£o simplificada...\")\n",
    "        for feature in top_feature_names:\n",
    "            try:\n",
    "                simple_result = simple_partial_dependence(model, X_train, feature)\n",
    "                if simple_result['success']:\n",
    "                    pd_results[feature] = simple_result\n",
    "                    print(f\"   ‚úÖ {feature}: implementa√ß√£o simples\")\n",
    "                    \n",
    "            except Exception as e:\n",
    "                print(f\"   ‚ùå {feature}: simples falhou - {e}\")\n",
    "                continue\n",
    "    \n",
    "    return pd_results\n",
    "\n",
    "def manual_partial_dependence(model, X_data, target_feature, feature_names, n_grid=15):\n",
    "    \"\"\"\n",
    "    Implementa√ß√£o manual de partial dependence.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Obter valores da feature\n",
    "        feature_values = X_data[target_feature]\n",
    "        \n",
    "        # Criar grid de valores\n",
    "        feature_min = feature_values.min()\n",
    "        feature_max = feature_values.max()\n",
    "        \n",
    "        # Criar grid com base nos percentis para melhor distribui√ß√£o\n",
    "        percentiles = np.linspace(5, 95, n_grid)\n",
    "        grid_values = np.percentile(feature_values, percentiles)\n",
    "        \n",
    "        # Garantir que temos valores √∫nicos\n",
    "        grid_values = np.unique(grid_values)\n",
    "        \n",
    "        print(f\"     üìä Grid: {feature_min:.3f} a {feature_max:.3f} ({len(grid_values)} pontos)\")\n",
    "        \n",
    "        # Calcular partial dependence\n",
    "        pd_values = []\n",
    "        \n",
    "        # Usar uma amostra menor para efici√™ncia\n",
    "        sample_size = min(500, len(X_data))\n",
    "        X_sample = X_data.sample(sample_size, random_state=RANDOM_STATE)\n",
    "        \n",
    "        for grid_val in grid_values:\n",
    "            # Criar dataset modificado\n",
    "            X_modified = X_sample.copy()\n",
    "            X_modified[target_feature] = grid_val\n",
    "            \n",
    "            # Fazer predi√ß√µes\n",
    "            if hasattr(model, 'predict_proba'):\n",
    "                predictions = model.predict_proba(X_modified)[:, 1]\n",
    "            else:\n",
    "                predictions = model.predict(X_modified)\n",
    "            \n",
    "            # M√©dia das predi√ß√µes\n",
    "            pd_values.append(predictions.mean())\n",
    "        \n",
    "        return {\n",
    "            'values': np.array(pd_values),\n",
    "            'grid': grid_values,\n",
    "            'feature_idx': feature_names.index(target_feature),\n",
    "            'method': 'manual',\n",
    "            'success': True\n",
    "        }\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"     ‚ùå Erro manual: {e}\")\n",
    "        return {'success': False}\n",
    "\n",
    "def simple_partial_dependence(model, X_data, target_feature, n_points=10):\n",
    "    \"\"\"\n",
    "    Implementa√ß√£o muito simples de partial dependence.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Valores b√°sicos da feature\n",
    "        feature_values = X_data[target_feature]\n",
    "        grid_values = np.linspace(feature_values.min(), feature_values.max(), n_points)\n",
    "        \n",
    "        # Usar apenas primeiras 100 amostras para efici√™ncia\n",
    "        X_simple = X_data.head(min(100, len(X_data)))\n",
    "        \n",
    "        pd_values = []\n",
    "        for val in grid_values:\n",
    "            # Modificar apenas a feature target\n",
    "            X_mod = X_simple.copy()\n",
    "            X_mod[target_feature] = val\n",
    "            \n",
    "            # Predi√ß√£o simples\n",
    "            if hasattr(model, 'predict_proba'):\n",
    "                pred = model.predict_proba(X_mod)[:, 1].mean()\n",
    "            else:\n",
    "                pred = model.predict(X_mod).mean()\n",
    "                \n",
    "            pd_values.append(pred)\n",
    "        \n",
    "        return {\n",
    "            'values': np.array(pd_values),\n",
    "            'grid': grid_values,\n",
    "            'method': 'simple',\n",
    "            'success': True\n",
    "        }\n",
    "        \n",
    "    except Exception as e:\n",
    "        return {'success': False}\n",
    "\n",
    "# Executar an√°lise robusta\n",
    "pd_results = robust_partial_dependence_analysis(\n",
    "    best_model, X_train_scaled, X.columns.tolist(), top_features=6\n",
    ")\n",
    "\n",
    "print(f\"\\nüìà PARTIAL DEPENDENCE ANALYSIS ROBUSTA:\")\n",
    "print(f\"   üìä Features analisadas com sucesso: {len(pd_results)}\")\n",
    "\n",
    "if pd_results:\n",
    "    print(f\"\\nüîç TEND√äNCIAS IDENTIFICADAS:\")\n",
    "    \n",
    "    # An√°lise das tend√™ncias\n",
    "    trends_summary = {}\n",
    "    \n",
    "    for feature, pd_data in pd_results.items():\n",
    "        values = pd_data['values']\n",
    "        grid = pd_data['grid']\n",
    "        \n",
    "        if len(values) > 2:\n",
    "            # Calcular tend√™ncia geral\n",
    "            slope = np.polyfit(range(len(values)), values, 1)[0]\n",
    "            \n",
    "            # Calcular mudan√ßa total\n",
    "            total_change = values[-1] - values[0]\n",
    "            \n",
    "            # Calcular volatilidade\n",
    "            volatility = np.std(np.diff(values))\n",
    "            \n",
    "            # Determinar dire√ß√£o\n",
    "            if slope > 0.01:\n",
    "                trend_direction = \"üìà Positiva\"\n",
    "            elif slope < -0.01:\n",
    "                trend_direction = \"üìâ Negativa\"\n",
    "            else:\n",
    "                trend_direction = \"‚û°Ô∏è Est√°vel\"\n",
    "            \n",
    "            # Calcular mudan√ßa percentual segura\n",
    "            if abs(values[0]) > 1e-10:\n",
    "                percent_change = (total_change / abs(values[0])) * 100\n",
    "            else:\n",
    "                percent_change = 0\n",
    "            \n",
    "            trends_summary[feature] = {\n",
    "                'direction': trend_direction,\n",
    "                'slope': slope,\n",
    "                'total_change': total_change,\n",
    "                'percent_change': percent_change,\n",
    "                'volatility': volatility,\n",
    "                'method': pd_data.get('method', 'unknown')\n",
    "            }\n",
    "            \n",
    "            print(f\"   {trend_direction} {feature}:\")\n",
    "            print(f\"     üí´ Mudan√ßa total: {total_change:+.3f}\")\n",
    "            print(f\"     üìä Mudan√ßa percentual: {percent_change:+.1f}%\")\n",
    "            print(f\"     üåä Volatilidade: {volatility:.3f}\")\n",
    "            print(f\"     üîß M√©todo: {pd_data.get('method', 'unknown')}\")\n",
    "        else:\n",
    "            print(f\"   ‚ö†Ô∏è {feature}: dados insuficientes para an√°lise de tend√™ncia\")\n",
    "            trends_summary[feature] = {\n",
    "                'direction': '‚ùì Indeterminada',\n",
    "                'method': pd_data.get('method', 'unknown')\n",
    "            }\n",
    "    \n",
    "    # Salvar para uso posterior\n",
    "    globals()['trends_summary'] = trends_summary\n",
    "    \n",
    "    # Identificar features com maior impacto\n",
    "    if trends_summary:\n",
    "        high_impact_features = []\n",
    "        for feature, trend in trends_summary.items():\n",
    "            if 'total_change' in trend and abs(trend['total_change']) > 0.05:\n",
    "                high_impact_features.append(feature)\n",
    "        \n",
    "        if high_impact_features:\n",
    "            print(f\"\\nüéØ FEATURES DE ALTO IMPACTO ({len(high_impact_features)}):\")\n",
    "            for feature in high_impact_features:\n",
    "                trend = trends_summary[feature]\n",
    "                print(f\"   üèÜ {feature}: {trend['direction']} ({trend['total_change']:+.3f})\")\n",
    "        else:\n",
    "            print(f\"\\nüìä Todas as features mostram impacto moderado nas predi√ß√µes\")\n",
    "            \n",
    "else:\n",
    "    print(\"‚ö†Ô∏è Nenhuma an√°lise de partial dependence dispon√≠vel\")\n",
    "    trends_summary = {}\n",
    "\n",
    "# Salvar vari√°veis para uso posterior\n",
    "globals()['pd_results'] = pd_results\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üìä 3. Visualiza√ß√µes de Interpretabilidade"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-15T20:53:23.917006Z",
     "iopub.status.busy": "2026-01-15T20:53:23.917006Z",
     "iopub.status.idle": "2026-01-15T20:53:24.470796Z",
     "shell.execute_reply": "2026-01-15T20:53:24.470282Z"
    }
   },
   "outputs": [],
   "source": [
    "# Criar visualiza√ß√µes de interpretabilidade robustas\n",
    "print_section(\"VISUALIZA√á√ïES DE INTERPRETABILIDADE\")\n",
    "\n",
    "def create_robust_interpretation_visualizations():\n",
    "    \"\"\"\n",
    "    Cria visualiza√ß√µes robustas de interpretabilidade.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Configurar subplot\n",
    "        fig, axes = plt.subplots(2, 3, figsize=(20, 12))\n",
    "        fig.suptitle('Model Interpretability Analysis', fontsize=16, y=0.98)\n",
    "        \n",
    "        # 1. Feature Importance Comparison\n",
    "        ax1 = axes[0, 0]\n",
    "        if len(feature_importance_results) > 0:\n",
    "            # Plotar m√∫ltiplos m√©todos se dispon√≠vel\n",
    "            colors = ['skyblue', 'lightcoral', 'lightgreen', 'gold']\n",
    "            \n",
    "            for i, (method, importance) in enumerate(feature_importance_results.items()):\n",
    "                top_features = importance.head(8)\n",
    "                \n",
    "                y_positions = np.arange(len(top_features)) + i * 0.2\n",
    "                bars = ax1.barh(y_positions, top_features.values, \n",
    "                              height=0.15, alpha=0.8, \n",
    "                              color=colors[i % len(colors)],\n",
    "                              label=method.title())\n",
    "            \n",
    "            ax1.set_yticks(np.arange(len(top_features)))\n",
    "            ax1.set_yticklabels([f.replace('_', ' ').title()[:15] for f in top_features.index], fontsize=9)\n",
    "            ax1.set_xlabel('Importance Score')\n",
    "            ax1.set_title('Feature Importance Comparison', fontsize=12, pad=15)\n",
    "            ax1.legend(fontsize=9)\n",
    "            ax1.grid(axis='x', alpha=0.3)\n",
    "            ax1.invert_yaxis()\n",
    "        else:\n",
    "            ax1.text(0.5, 0.5, 'Feature Importance\\nNot Available', \n",
    "                    ha='center', va='center', transform=ax1.transAxes,\n",
    "                    fontsize=12, bbox=dict(boxstyle='round', facecolor='lightgray', alpha=0.8))\n",
    "            ax1.set_title('Feature Importance', fontsize=12, pad=15)\n",
    "        \n",
    "        # 2. Partial Dependence Plots\n",
    "        ax2 = axes[0, 1]\n",
    "        if len(pd_results) > 0:\n",
    "            # Plotar top 4 partial dependence\n",
    "            colors = plt.cm.Set1(np.linspace(0, 1, len(pd_results)))\n",
    "            \n",
    "            for i, (feature, pd_data) in enumerate(list(pd_results.items())[:4]):\n",
    "                grid = pd_data['grid']\n",
    "                values = pd_data['values']\n",
    "                ax2.plot(grid, values, 'o-', color=colors[i], \n",
    "                        label=feature.replace('_', ' ')[:10], linewidth=2, markersize=4)\n",
    "            \n",
    "            ax2.set_xlabel('Feature Value')\n",
    "            ax2.set_ylabel('Partial Dependence')\n",
    "            ax2.set_title('Partial Dependence Analysis', fontsize=12, pad=15)\n",
    "            ax2.legend(fontsize=9)\n",
    "            ax2.grid(alpha=0.3)\n",
    "        else:\n",
    "            ax2.text(0.5, 0.5, 'Partial Dependence\\nNot Available', \n",
    "                    ha='center', va='center', transform=ax2.transAxes,\n",
    "                    fontsize=12, bbox=dict(boxstyle='round', facecolor='lightgray', alpha=0.8))\n",
    "            ax2.set_title('Partial Dependence', fontsize=12, pad=15)\n",
    "        \n",
    "        # 3. Model Performance Metrics\n",
    "        ax3 = axes[0, 2]\n",
    "        \n",
    "        # Calcular m√©tricas b√°sicas\n",
    "        y_pred = best_model.predict(X_test_scaled)\n",
    "        y_prob = best_model.predict_proba(X_test_scaled)[:, 1] if hasattr(best_model, 'predict_proba') else y_pred\n",
    "        \n",
    "        from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score\n",
    "        \n",
    "        metrics = {\n",
    "            'Accuracy': accuracy_score(y_test, y_pred),\n",
    "            'Precision': precision_score(y_test, y_pred),\n",
    "            'Recall': recall_score(y_test, y_pred),\n",
    "            'F1-Score': f1_score(y_test, y_pred),\n",
    "            'AUC-ROC': roc_auc_score(y_test, y_prob)\n",
    "        }\n",
    "        \n",
    "        metric_names = list(metrics.keys())\n",
    "        metric_values = list(metrics.values())\n",
    "        colors_metrics = plt.cm.viridis(np.linspace(0, 1, len(metrics)))\n",
    "        \n",
    "        bars = ax3.bar(metric_names, metric_values, color=colors_metrics, alpha=0.8)\n",
    "        ax3.set_ylim(0, 1)\n",
    "        ax3.set_title('Model Performance Metrics', fontsize=12, pad=15)\n",
    "        ax3.set_ylabel('Score')\n",
    "        plt.setp(ax3.get_xticklabels(), rotation=45, ha='right')\n",
    "        \n",
    "        # Adicionar valores nas barras\n",
    "        for bar, value in zip(bars, metric_values):\n",
    "            height = bar.get_height()\n",
    "            ax3.text(bar.get_x() + bar.get_width()/2., height + 0.01,\n",
    "                    f'{value:.3f}', ha='center', va='bottom', fontsize=9)\n",
    "        \n",
    "        ax3.grid(axis='y', alpha=0.3)\n",
    "        \n",
    "        # 4. Confusion Matrix\n",
    "        ax4 = axes[1, 0]\n",
    "        from sklearn.metrics import confusion_matrix\n",
    "        \n",
    "        cm = confusion_matrix(y_test, y_pred)\n",
    "        sns.heatmap(cm, annot=True, fmt='d', cmap='Blues',\n",
    "                   xticklabels=['No Risk', 'Risk'],\n",
    "                   yticklabels=['No Risk', 'Risk'], ax=ax4)\n",
    "        ax4.set_title('Confusion Matrix', fontsize=12, pad=15)\n",
    "        ax4.set_ylabel('True Label')\n",
    "        ax4.set_xlabel('Predicted Label')\n",
    "        \n",
    "        # 5. Prediction Distribution\n",
    "        ax5 = axes[1, 1]\n",
    "        \n",
    "        # Distribui√ß√£o de probabilidades por classe\n",
    "        prob_class_0 = y_prob[y_test == 0]\n",
    "        prob_class_1 = y_prob[y_test == 1]\n",
    "        \n",
    "        ax5.hist(prob_class_0, bins=30, alpha=0.7, label='No Hypertension Risk', \n",
    "                color='lightblue', density=True)\n",
    "        ax5.hist(prob_class_1, bins=30, alpha=0.7, label='Hypertension Risk', \n",
    "                color='lightcoral', density=True)\n",
    "        ax5.axvline(x=0.5, color='red', linestyle='--', alpha=0.7, label='Threshold 0.5')\n",
    "        \n",
    "        ax5.set_xlabel('Predicted Probability')\n",
    "        ax5.set_ylabel('Density')\n",
    "        ax5.set_title('Prediction Distribution', fontsize=12, pad=15)\n",
    "        ax5.legend(fontsize=9)\n",
    "        ax5.grid(alpha=0.3)\n",
    "        \n",
    "        # 6. Feature Categories Summary\n",
    "        ax6 = axes[1, 2]\n",
    "        \n",
    "        # An√°lise por categorias se dispon√≠vel\n",
    "        if 'category_importance' in globals() and len(category_importance) > 0:\n",
    "            categories = list(category_importance.keys())\n",
    "            importance_means = [data['mean_importance'] for data in category_importance.values()]\n",
    "            \n",
    "            colors_cat = plt.cm.Set3(np.linspace(0, 1, len(categories)))\n",
    "            \n",
    "            wedges, texts, autotexts = ax6.pie(importance_means, labels=categories, autopct='%1.1f%%',\n",
    "                                              colors=colors_cat, startangle=90)\n",
    "            ax6.set_title('Feature Categories\\nImportance Distribution', fontsize=12, pad=15)\n",
    "            \n",
    "            # Ajustar tamanho do texto\n",
    "            for text in texts:\n",
    "                text.set_fontsize(8)\n",
    "            for autotext in autotexts:\n",
    "                autotext.set_fontsize(8)\n",
    "                autotext.set_color('white')\n",
    "        else:\n",
    "            ax6.text(0.5, 0.5, 'Feature Categories\\nNot Available', \n",
    "                    ha='center', va='center', transform=ax6.transAxes,\n",
    "                    fontsize=12, bbox=dict(boxstyle='round', facecolor='lightgray', alpha=0.8))\n",
    "            ax6.set_title('Feature Categories', fontsize=12, pad=15)\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        \n",
    "        # Salvar figura\n",
    "        try:\n",
    "            save_figure('model_interpretability_analysis')\n",
    "            print(\"üíæ Visualiza√ß√µes salvas como: model_interpretability_analysis.png\")\n",
    "        except Exception as e:\n",
    "            print(f\"‚ö†Ô∏è Erro ao salvar figura: {e}\")\n",
    "        \n",
    "        plt.show()\n",
    "        \n",
    "        return True\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Erro ao criar visualiza√ß√µes: {e}\")\n",
    "        return False\n",
    "\n",
    "# Executar cria√ß√£o de visualiza√ß√µes\n",
    "success = create_robust_interpretation_visualizations()\n",
    "\n",
    "if success:\n",
    "    print(\"\\n‚úÖ Visualiza√ß√µes de interpretabilidade criadas e salvas!\")\n",
    "else:\n",
    "    print(\"\\n‚ö†Ô∏è Algumas visualiza√ß√µes podem n√£o ter sido criadas corretamente\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üè• 4. An√°lise M√©dica Detalhada"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-15T20:53:24.472804Z",
     "iopub.status.busy": "2026-01-15T20:53:24.472804Z",
     "iopub.status.idle": "2026-01-15T20:53:24.488900Z",
     "shell.execute_reply": "2026-01-15T20:53:24.488900Z"
    }
   },
   "outputs": [],
   "source": [
    "print_section(\"AN√ÅLISE M√âDICA DETALHADA\")\n",
    "\n",
    "# Implementar an√°lise m√©dica robusta\n",
    "def robust_medical_analysis(X_test, y_test, best_model, target_col):\n",
    "    \"\"\"\n",
    "    An√°lise m√©dica robusta dos resultados.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Preparar dados com predi√ß√µes e probabilidades\n",
    "        df_analysis = X_test.copy()\n",
    "        df_analysis[target_col] = y_test\n",
    "        df_analysis['predicted'] = best_model.predict(X_test_scaled)\n",
    "        \n",
    "        if hasattr(best_model, 'predict_proba'):\n",
    "            df_analysis['probability'] = best_model.predict_proba(X_test_scaled)[:, 1]\n",
    "        else:\n",
    "            df_analysis['probability'] = best_model.decision_function(X_test_scaled)\n",
    "            # Normalizar para [0,1] se necess√°rio\n",
    "            if df_analysis['probability'].min() < 0:\n",
    "                prob_min = df_analysis['probability'].min()\n",
    "                prob_max = df_analysis['probability'].max()\n",
    "                df_analysis['probability'] = (df_analysis['probability'] - prob_min) / (prob_max - prob_min)\n",
    "        \n",
    "        print(f\"‚úÖ Dados preparados para an√°lise m√©dica\")\n",
    "        print(f\"   üìä Amostras: {len(df_analysis):,}\")\n",
    "        print(f\"   üéØ Accuracy: {(df_analysis[target_col] == df_analysis['predicted']).mean():.3f}\")\n",
    "        \n",
    "        # Criar relat√≥rio m√©dico robusto\n",
    "        medical_report = create_comprehensive_medical_report(df_analysis, target_col)\n",
    "        \n",
    "        print(f\"\\nüìã RELAT√ìRIO M√âDICO COMPLETO GERADO\")\n",
    "        print(f\"   üìä Se√ß√µes inclu√≠das: {len(medical_report)}\")\n",
    "        \n",
    "        return df_analysis, medical_report\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Erro na an√°lise m√©dica: {e}\")\n",
    "        return None, None\n",
    "\n",
    "def create_comprehensive_medical_report(df_analysis, target_col):\n",
    "    \"\"\"\n",
    "    Cria relat√≥rio m√©dico abrangente.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Estat√≠sticas b√°sicas\n",
    "        total_pacientes = len(df_analysis)\n",
    "        prevalencia_hipertensao = (df_analysis[target_col].sum() / total_pacientes) * 100\n",
    "        \n",
    "        # Idade m√©dia (se dispon√≠vel)\n",
    "        idade_media = 0\n",
    "        if 'idade' in df_analysis.columns:\n",
    "            idade_media = df_analysis['idade'].mean()\n",
    "        elif any('idade' in col for col in df_analysis.columns):\n",
    "            # Procurar colunas relacionadas √† idade\n",
    "            idade_cols = [col for col in df_analysis.columns if 'idade' in col.lower()]\n",
    "            if idade_cols:\n",
    "                idade_media = df_analysis[idade_cols[0]].mean()\n",
    "        \n",
    "        # An√°lise de performance cl√≠nica\n",
    "        y_true = df_analysis[target_col]\n",
    "        y_pred = df_analysis['predicted'] \n",
    "        y_prob = df_analysis['probability']\n",
    "        \n",
    "        # M√©tricas m√©dicas\n",
    "        from sklearn.metrics import confusion_matrix, classification_report\n",
    "        \n",
    "        cm = confusion_matrix(y_true, y_pred)\n",
    "        \n",
    "        # Calcular m√©tricas cl√≠nicas\n",
    "        tn, fp, fn, tp = cm.ravel()\n",
    "        \n",
    "        sensitivity = tp / (tp + fn) if (tp + fn) > 0 else 0  # Recall\n",
    "        specificity = tn / (tn + fp) if (tn + fp) > 0 else 0\n",
    "        ppv = tp / (tp + fp) if (tp + fp) > 0 else 0  # Precision\n",
    "        npv = tn / (tn + fn) if (tn + fn) > 0 else 0\n",
    "        \n",
    "        # An√°lise de distribui√ß√£o de probabilidades\n",
    "        prob_stats = {\n",
    "            'media': y_prob.mean(),\n",
    "            'mediana': y_prob.median(),\n",
    "            'std': y_prob.std(),\n",
    "            'quartil_25': y_prob.quantile(0.25),\n",
    "            'quartil_75': y_prob.quantile(0.75)\n",
    "        }\n",
    "        \n",
    "        # An√°lise por grupos de risco\n",
    "        risk_groups = {\n",
    "            'baixo': (y_prob <= 0.3).sum(),\n",
    "            'moderado': ((y_prob > 0.3) & (y_prob <= 0.7)).sum(),\n",
    "            'alto': (y_prob > 0.7).sum()\n",
    "        }\n",
    "        \n",
    "        # Compilar relat√≥rio\n",
    "        medical_report = {\n",
    "            'dados_gerais': {\n",
    "                'total_pacientes': total_pacientes,\n",
    "                'prevalencia_hipertensao': prevalencia_hipertensao,\n",
    "                'idade_media': idade_media\n",
    "            },\n",
    "            'metricas_clinicas': {\n",
    "                'sensibilidade': sensitivity,\n",
    "                'especificidade': specificity,\n",
    "                'valor_preditivo_positivo': ppv,\n",
    "                'valor_preditivo_negativo': npv,\n",
    "                'acuracia': (tp + tn) / (tp + tn + fp + fn) if (tp + tn + fp + fn) > 0 else 0\n",
    "            },\n",
    "            'confusion_matrix': {\n",
    "                'verdadeiro_negativo': int(tn),\n",
    "                'falso_positivo': int(fp),\n",
    "                'falso_negativo': int(fn),\n",
    "                'verdadeiro_positivo': int(tp)\n",
    "            },\n",
    "            'analise_probabilidades': prob_stats,\n",
    "            'grupos_risco': risk_groups,\n",
    "            'sindrome_metabolica': {\n",
    "                'prevalencia_sindrome': 0  # Placeholder - seria calculado com dados espec√≠ficos\n",
    "            }\n",
    "        }\n",
    "        \n",
    "        return medical_report\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Erro ao criar relat√≥rio m√©dico: {e}\")\n",
    "        return {}\n",
    "\n",
    "# Executar an√°lise m√©dica\n",
    "df_analysis, medical_report = robust_medical_analysis(X_test, y_test, best_model, target_col)\n",
    "\n",
    "if medical_report:\n",
    "    # Exibir resumo do relat√≥rio\n",
    "    print(f\"\\nüè• RESUMO M√âDICO:\")\n",
    "    print(f\"   üë• Total de pacientes: {medical_report['dados_gerais']['total_pacientes']:,}\")\n",
    "    print(f\"   üìà Preval√™ncia hipertens√£o: {medical_report['dados_gerais']['prevalencia_hipertensao']:.1f}%\")\n",
    "    print(f\"   ü©∫ Sensibilidade: {medical_report['metricas_clinicas']['sensibilidade']:.1%}\")\n",
    "    print(f\"   üõ°Ô∏è Especificidade: {medical_report['metricas_clinicas']['especificidade']:.1%}\")\n",
    "    print(f\"   üíä Valor Preditivo Positivo: {medical_report['metricas_clinicas']['valor_preditivo_positivo']:.1%}\")\n",
    "    \n",
    "    # An√°lise de grupos de risco\n",
    "    print(f\"\\nüìä DISTRIBUI√á√ÉO DE RISCO:\")\n",
    "    total_patients = sum(medical_report['grupos_risco'].values())\n",
    "    for grupo, count in medical_report['grupos_risco'].items():\n",
    "        percentage = (count / total_patients * 100) if total_patients > 0 else 0\n",
    "        print(f\"   üéØ Risco {grupo}: {count} pacientes ({percentage:.1f}%)\")\n",
    "        \n",
    "# Salvar vari√°veis para uso posterior\n",
    "globals()['df_analysis'] = df_analysis\n",
    "globals()['medical_report'] = medical_report\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### üîç 4.1 Feature Importance vs Clinical Relevance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-15T20:53:24.491441Z",
     "iopub.status.busy": "2026-01-15T20:53:24.491441Z",
     "iopub.status.idle": "2026-01-15T20:53:24.504020Z",
     "shell.execute_reply": "2026-01-15T20:53:24.504020Z"
    }
   },
   "outputs": [],
   "source": [
    "print_section(\"IMPORT√ÇNCIA DAS FEATURES vs RELEV√ÇNCIA CL√çNICA\")\n",
    "\n",
    "# Mapear features para categorias cl√≠nicas\n",
    "clinical_categories = {\n",
    "    'Press√£o Arterial': ['pressao_sistolica', 'pressao_diastolica', 'pressao_arterial_media', \n",
    "                         'pressao_pulso', 'categoria_pa', 'pam'],\n",
    "    'Antropom√©tricas': ['imc', 'peso', 'altura', 'bsa', 'categoria_imc'],\n",
    "    'Risco Cardiovascular': ['framingham', 'score_risco', 'risco_cv', 'sindrome_metabolica'],\n",
    "    'Biomarcadores': ['colesterol_total', 'hdl', 'ldl', 'triglicerides', 'glicose'],\n",
    "    'Demografia': ['idade', 'sexo', 'decada', 'faixa_etaria'],\n",
    "    'Estilo de Vida': ['fumante', 'atividade_fisica', 'alcool'],\n",
    "    'Medicamentos': ['medicamento_pressao', 'medicamento_colesterol', 'diabetes'],\n",
    "    'Features Engineered': ['interacao', 'composite', 'ratio']\n",
    "}\n",
    "\n",
    "# Analisar import√¢ncia por categoria cl√≠nica\n",
    "if 'intrinsic' in feature_importance_results:\n",
    "    importance_scores = feature_importance_results['intrinsic']\n",
    "elif 'permutation' in feature_importance_results:\n",
    "    importance_scores = feature_importance_results['permutation']\n",
    "else:\n",
    "    importance_scores = list(feature_importance_results.values())[0]\n",
    "\n",
    "category_importance = {}\n",
    "for category, keywords in clinical_categories.items():\n",
    "    relevant_features = []\n",
    "    for feature in importance_scores.index:\n",
    "        if any(keyword.lower() in feature.lower() for keyword in keywords):\n",
    "            relevant_features.append(feature)\n",
    "    \n",
    "    if relevant_features:\n",
    "        category_scores = importance_scores[relevant_features]\n",
    "        category_importance[category] = {\n",
    "            'mean_importance': category_scores.mean(),\n",
    "            'max_importance': category_scores.max(),\n",
    "            'n_features': len(category_scores),\n",
    "            'top_feature': category_scores.idxmax(),\n",
    "            'top_score': category_scores.max()\n",
    "        }\n",
    "\n",
    "# Mostrar an√°lise por categoria\n",
    "print(\"\\nüè• IMPORT√ÇNCIA POR CATEGORIA CL√çNICA:\")\n",
    "category_df = pd.DataFrame(category_importance).T\n",
    "category_df_sorted = category_df.sort_values('mean_importance', ascending=False)\n",
    "\n",
    "for i, (category, data) in enumerate(category_df_sorted.iterrows(), 1):\n",
    "    print(f\"  {i}. {category}:\")\n",
    "    print(f\"     üìä Import√¢ncia m√©dia: {data['mean_importance']:.4f}\")\n",
    "    print(f\"     üèÜ Feature principal: {data['top_feature']} ({data['top_score']:.4f})\")\n",
    "    print(f\"     üî¢ N√∫mero de features: {data['n_features']:.0f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### üíä 4.2 Clinical Decision Support Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-15T20:53:24.507027Z",
     "iopub.status.busy": "2026-01-15T20:53:24.506026Z",
     "iopub.status.idle": "2026-01-15T20:53:24.625482Z",
     "shell.execute_reply": "2026-01-15T20:53:24.625482Z"
    }
   },
   "outputs": [],
   "source": [
    "print_section(\"AN?LISE DE SUPORTE ? DECIS?O CL?NICA\")\n",
    "import pickle\n",
    "\n",
    "# Garantir modelo carregado\n",
    "if best_model is None:\n",
    "    print(\"?? best_model nao encontrado. Tentando carregar artefatos...\")\n",
    "    for candidate in [MODELS_FINAL_DIR / 'best_model_optimized.pkl', MODELS_TRAINED_DIR / 'best_model.pkl']:\n",
    "        if candidate.exists():\n",
    "            with open(candidate, 'rb') as f:\n",
    "                best_model = pickle.load(f)\n",
    "            print(f\"? Modelo carregado: {candidate}\")\n",
    "            break\n",
    "\n",
    "if best_model is None:\n",
    "    print(\"?? Nao foi possivel carregar um modelo. Pulando analise de thresholds.\")\n",
    "    threshold_df = pd.DataFrame()\n",
    "else:\n",
    "    # Analisar performance por diferentes thresholds de probabilidade\n",
    "    from sklearn.metrics import precision_recall_curve, roc_curve\n",
    "\n",
    "    if hasattr(best_model, 'predict_proba'):\n",
    "        y_proba = best_model.predict_proba(X_test_scaled)[:, 1]\n",
    "    else:\n",
    "        y_proba = best_model.decision_function(X_test_scaled)\n",
    "        # Normalizar para [0,1]\n",
    "        y_proba = (y_proba - y_proba.min()) / (y_proba.max() - y_proba.min())\n",
    "\n",
    "    # Calcular metricas para diferentes thresholds\n",
    "    thresholds = [0.3, 0.4, 0.5, 0.6, 0.7, 0.8]\n",
    "    threshold_analysis = []\n",
    "\n",
    "    for threshold in thresholds:\n",
    "        y_pred_thresh = (y_proba >= threshold).astype(int)\n",
    "\n",
    "        # Calcular metricas\n",
    "        from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "\n",
    "        tn, fp, fn, tp = confusion_matrix(y_test, y_pred_thresh).ravel()\n",
    "\n",
    "        metrics = {\n",
    "            'threshold': threshold,\n",
    "            'accuracy': accuracy_score(y_test, y_pred_thresh),\n",
    "            'precision': precision_score(y_test, y_pred_thresh, zero_division=0),\n",
    "            'recall': recall_score(y_test, y_pred_thresh, zero_division=0),\n",
    "            'f1': f1_score(y_test, y_pred_thresh, zero_division=0),\n",
    "            'sensitivity': tp / (tp + fn) if (tp + fn) > 0 else 0,\n",
    "            'specificity': tn / (tn + fp) if (tn + fp) > 0 else 0,\n",
    "            'ppv': tp / (tp + fp) if (tp + fp) > 0 else 0,\n",
    "            'npv': tn / (tn + fn) if (tn + fn) > 0 else 0,\n",
    "            'false_positive_rate': fp / (fp + tn) if (fp + tn) > 0 else 0,\n",
    "            'false_negative_rate': fn / (fn + tp) if (fn + tp) > 0 else 0\n",
    "        }\n",
    "\n",
    "        threshold_analysis.append(metrics)\n",
    "\n",
    "    threshold_df = pd.DataFrame(threshold_analysis)\n",
    "\n",
    "    print(\"\\n?? AN?LISE DE THRESHOLDS PARA DECIS?O CL?NICA:\")\n",
    "    print(threshold_df.round(3))\n",
    "\n",
    "    # Recomendacoes clinicas\n",
    "    print(\"\\n?? RECOMENDA??ES DE THRESHOLD:\")\n",
    "\n",
    "    # Threshold para alta sensibilidade (screening)\n",
    "    high_sensitivity_idx = threshold_df['sensitivity'].idxmax()\n",
    "    high_sens_threshold = threshold_df.loc[high_sensitivity_idx]\n",
    "    print(f\"   ?? Para Screening (alta sensibilidade): {high_sens_threshold['threshold']:.1f}\")\n",
    "    print(f\"       Sensibilidade: {high_sens_threshold['sensitivity']:.1%}, Especificidade: {high_sens_threshold['specificity']:.1%}\")\n",
    "\n",
    "    # Threshold para alta especificidade (confirmacao)\n",
    "    high_specificity_idx = threshold_df['specificity'].idxmax()\n",
    "    high_spec_threshold = threshold_df.loc[high_specificity_idx]\n",
    "    print(f\"   ? Para Confirma??o (alta especificidade): {high_spec_threshold['threshold']:.1f}\")\n",
    "    print(f\"       Sensibilidade: {high_spec_threshold['sensitivity']:.1%}, Especificidade: {high_spec_threshold['specificity']:.1%}\")\n",
    "\n",
    "    # Threshold balanceado\n",
    "    balanced_idx = threshold_df['f1'].idxmax()\n",
    "    balanced_threshold = threshold_df.loc[balanced_idx]\n",
    "    print(f\"   ?? Balanceado (melhor F1): {balanced_threshold['threshold']:.1f}\")\n",
    "    print(f\"       Sensibilidade: {balanced_threshold['sensitivity']:.1%}, Especificidade: {balanced_threshold['specificity']:.1%}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üìä 5. Visualiza√ß√£o da Performance Cl√≠nica"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-15T20:53:24.628491Z",
     "iopub.status.busy": "2026-01-15T20:53:24.627489Z",
     "iopub.status.idle": "2026-01-15T20:53:26.889484Z",
     "shell.execute_reply": "2026-01-15T20:53:26.888477Z"
    }
   },
   "outputs": [],
   "source": [
    "print_section(\"VISUALIZA√á√ÉO DA PERFORMANCE CL√çNICA\")\n",
    "\n",
    "# Criar visualiza√ß√µes m√©dicas detalhadas\n",
    "fig, axes = plt.subplots(2, 3, figsize=(20, 12))\n",
    "fig.suptitle('Clinical Performance Analysis', fontsize=16, y=0.98)\n",
    "\n",
    "# 1. Threshold Analysis\n",
    "ax1 = axes[0, 0]\n",
    "ax1.plot(threshold_df['threshold'], threshold_df['sensitivity'], 'o-', label='Sensitivity', linewidth=2)\n",
    "ax1.plot(threshold_df['threshold'], threshold_df['specificity'], 's-', label='Specificity', linewidth=2)\n",
    "ax1.plot(threshold_df['threshold'], threshold_df['f1'], '^-', label='F1-Score', linewidth=2)\n",
    "ax1.set_xlabel('Probability Threshold')\n",
    "ax1.set_ylabel('Score')\n",
    "ax1.set_title('Threshold Analysis for Clinical Decision', fontsize=12, pad=15)\n",
    "ax1.legend()\n",
    "ax1.grid(alpha=0.3)\n",
    "\n",
    "# 2. Clinical Category Importance\n",
    "ax2 = axes[0, 1]\n",
    "category_scores = [data['mean_importance'] for data in category_importance.values()]\n",
    "category_names = list(category_importance.keys())\n",
    "\n",
    "bars = ax2.barh(range(len(category_scores)), category_scores, \n",
    "               color=plt.cm.Set3(np.linspace(0, 1, len(category_scores))))\n",
    "ax2.set_yticks(range(len(category_scores)))\n",
    "ax2.set_yticklabels(category_names, fontsize=9)\n",
    "ax2.set_xlabel('Mean Feature Importance')\n",
    "ax2.set_title('Clinical Category Importance', fontsize=12, pad=15)\n",
    "ax2.grid(axis='x', alpha=0.3)\n",
    "ax2.invert_yaxis()\n",
    "\n",
    "# 3. ROC Curve with Clinical Thresholds\n",
    "ax3 = axes[0, 2]\n",
    "fpr, tpr, roc_thresholds = roc_curve(y_test, y_proba)\n",
    "ax3.plot(fpr, tpr, 'b-', linewidth=2, label=f'ROC (AUC: {best_auc:.3f})')\n",
    "ax3.plot([0, 1], [0, 1], 'k--', alpha=0.5)\n",
    "\n",
    "# Marcar thresholds cl√≠nicos\n",
    "for i, threshold in enumerate([0.3, 0.5, 0.7]):\n",
    "    y_pred_thresh = (y_proba >= threshold).astype(int)\n",
    "    tn, fp, fn, tp = confusion_matrix(y_test, y_pred_thresh).ravel()\n",
    "    fpr_point = fp / (fp + tn) if (fp + tn) > 0 else 0\n",
    "    tpr_point = tp / (tp + fn) if (tp + fn) > 0 else 0\n",
    "    ax3.plot(fpr_point, tpr_point, 'ro', markersize=8, label=f'Threshold {threshold}')\n",
    "\n",
    "ax3.set_xlabel('False Positive Rate')\n",
    "ax3.set_ylabel('True Positive Rate')\n",
    "ax3.set_title('ROC with Clinical Thresholds', fontsize=12, pad=15)\n",
    "ax3.legend(fontsize=9)\n",
    "ax3.grid(alpha=0.3)\n",
    "\n",
    "# 4. Distribution of Predictions by True Class\n",
    "ax4 = axes[1, 0]\n",
    "# Predi√ß√µes para cada classe\n",
    "prob_class_0 = y_proba[y_test == 0]\n",
    "prob_class_1 = y_proba[y_test == 1]\n",
    "\n",
    "ax4.hist(prob_class_0, bins=30, alpha=0.7, label='No Hypertension', color='lightblue', density=True)\n",
    "ax4.hist(prob_class_1, bins=30, alpha=0.7, label='Hypertension', color='lightcoral', density=True)\n",
    "ax4.axvline(x=0.5, color='red', linestyle='--', alpha=0.7, label='Default Threshold')\n",
    "ax4.set_xlabel('Predicted Probability')\n",
    "ax4.set_ylabel('Density')\n",
    "ax4.set_title('Prediction Distribution by True Class', fontsize=12, pad=15)\n",
    "ax4.legend()\n",
    "ax4.grid(alpha=0.3)\n",
    "\n",
    "# 5. Precision-Recall Curve\n",
    "ax5 = axes[1, 1]\n",
    "precision, recall, pr_thresholds = precision_recall_curve(y_test, y_proba)\n",
    "ax5.plot(recall, precision, 'g-', linewidth=2, label='Precision-Recall')\n",
    "ax5.axhline(y=y_test.mean(), color='red', linestyle='--', alpha=0.7, \n",
    "           label=f'Baseline ({y_test.mean():.3f})')\n",
    "\n",
    "ax5.set_xlabel('Recall (Sensitivity)')\n",
    "ax5.set_ylabel('Precision (PPV)')\n",
    "ax5.set_title('Precision-Recall Curve', fontsize=12, pad=15)\n",
    "ax5.legend()\n",
    "ax5.grid(alpha=0.3)\n",
    "\n",
    "# 6. Clinical Metrics Heatmap\n",
    "ax6 = axes[1, 2]\n",
    "clinical_metrics = threshold_df[['threshold', 'sensitivity', 'specificity', 'ppv', 'npv']].set_index('threshold')\n",
    "sns.heatmap(clinical_metrics.T, annot=True, cmap='RdYlBu_r', center=0.5, \n",
    "           fmt='.2f', cbar_kws={'shrink': 0.8}, ax=ax6)\n",
    "ax6.set_title('Clinical Metrics by Threshold', fontsize=12, pad=15)\n",
    "ax6.set_xlabel('Threshold')\n",
    "ax6.set_ylabel('Clinical Metric')\n",
    "\n",
    "plt.tight_layout()\n",
    "save_figure('clinical_performance_analysis')\n",
    "plt.show()\n",
    "\n",
    "print(\"\\n‚úÖ Visualiza√ß√µes de performance cl√≠nica criadas e salvas!\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üìã 6. Gera√ß√£o do Relat√≥rio Final de Interpretabilidade"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-15T20:53:26.891579Z",
     "iopub.status.busy": "2026-01-15T20:53:26.891579Z",
     "iopub.status.idle": "2026-01-15T20:53:27.053418Z",
     "shell.execute_reply": "2026-01-15T20:53:27.053418Z"
    }
   },
   "outputs": [],
   "source": [
    "print_section(\"GERANDO RELAT√ìRIO FINAL DE INTERPRETABILIDADE\")\n",
    "\n",
    "# Fun√ß√£o para gerar relat√≥rio robusto\n",
    "def generate_robust_interpretation_report():\n",
    "    \"\"\"\n",
    "    Gera relat√≥rio de interpretabilidade robusto.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Compilar informa√ß√µes b√°sicas\n",
    "        basic_info = {\n",
    "            'timestamp': datetime.now().isoformat(),\n",
    "            'model_type': type(best_model).__name__,\n",
    "            'n_features': len(X.columns),\n",
    "            'n_samples_train': len(X_train),\n",
    "            'n_samples_test': len(X_test),\n",
    "            'target_column': target_col\n",
    "        }\n",
    "        \n",
    "        # Feature importance (se dispon√≠vel)\n",
    "        feature_analysis = {}\n",
    "        if 'feature_importance_results' in globals():\n",
    "            feature_analysis = {\n",
    "                'methods_available': list(feature_importance_results.keys()),\n",
    "                'top_features': {}\n",
    "            }\n",
    "            \n",
    "            for method, importance in feature_importance_results.items():\n",
    "                feature_analysis['top_features'][method] = importance.head(10).to_dict()\n",
    "        \n",
    "        # Partial dependence (se dispon√≠vel)\n",
    "        pd_analysis = {}\n",
    "        if 'pd_results' in globals():\n",
    "            pd_analysis = {\n",
    "                'features_analyzed': list(pd_results.keys()),\n",
    "                'summary': {}\n",
    "            }\n",
    "            \n",
    "            for feature, pd_data in pd_results.items():\n",
    "                values = pd_data['values']\n",
    "                if len(values) > 1:\n",
    "                    trend = \"positive\" if values[-1] > values[0] else \"negative\"\n",
    "                    change = ((values[-1] - values[0]) / abs(values[0])) * 100 if abs(values[0]) > 1e-10 else 0\n",
    "                else:\n",
    "                    trend = \"unknown\"\n",
    "                    change = 0\n",
    "                \n",
    "                pd_analysis['summary'][feature] = {\n",
    "                    'trend': trend,\n",
    "                    'change_percent': change\n",
    "                }\n",
    "        \n",
    "        # Performance metrics\n",
    "        y_pred = best_model.predict(X_test_scaled)\n",
    "        y_prob = best_model.predict_proba(X_test_scaled)[:, 1] if hasattr(best_model, 'predict_proba') else y_pred\n",
    "        \n",
    "        from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score\n",
    "        \n",
    "        performance_metrics = {\n",
    "            'accuracy': accuracy_score(y_test, y_pred),\n",
    "            'precision': precision_score(y_test, y_pred),\n",
    "            'recall': recall_score(y_test, y_pred),\n",
    "            'f1_score': f1_score(y_test, y_pred),\n",
    "            'auc_roc': roc_auc_score(y_test, y_prob)\n",
    "        }\n",
    "        \n",
    "        # Compilar relat√≥rio final\n",
    "        interpretation_report = {\n",
    "            'basic_info': basic_info,\n",
    "            'feature_importance': feature_analysis,\n",
    "            'partial_dependence': pd_analysis,\n",
    "            'performance_metrics': performance_metrics,\n",
    "            'shap_analysis': {'available': SHAP_AVAILABLE, 'results': {}},\n",
    "            'generation_timestamp': datetime.now().isoformat()\n",
    "        }\n",
    "        \n",
    "        return interpretation_report\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Erro ao gerar relat√≥rio: {e}\")\n",
    "        return {}\n",
    "\n",
    "# Gerar relat√≥rio de interpretabilidade\n",
    "interpretation_report = generate_robust_interpretation_report()\n",
    "\n",
    "print(f\"‚úÖ Relat√≥rio de interpretabilidade gerado\")\n",
    "print(f\"   üìä Se√ß√µes inclu√≠das: {len(interpretation_report)}\")\n",
    "\n",
    "# Verificar disponibilidade de vari√°veis importantes\n",
    "required_vars = ['high_sens_threshold', 'high_spec_threshold', 'balanced_threshold', 'category_importance']\n",
    "missing_vars = []\n",
    "\n",
    "for var in required_vars:\n",
    "    if var not in globals():\n",
    "        missing_vars.append(var)\n",
    "\n",
    "if missing_vars:\n",
    "    print(f\"‚ö†Ô∏è Vari√°veis em falta: {missing_vars}\")\n",
    "    print(\"üîß Recriando vari√°veis necess√°rias...\")\n",
    "    \n",
    "    # Recriar threshold analysis se necess√°rio\n",
    "    if 'threshold_df' not in globals():\n",
    "        print(\"üîÑ Recalculando an√°lise de thresholds...\")\n",
    "        \n",
    "        # An√°lise b√°sica de thresholds\n",
    "        thresholds = [0.3, 0.4, 0.5, 0.6, 0.7, 0.8]\n",
    "        threshold_analysis = []\n",
    "        \n",
    "        y_prob = best_model.predict_proba(X_test_scaled)[:, 1] if hasattr(best_model, 'predict_proba') else best_model.predict(X_test_scaled)\n",
    "        \n",
    "        for threshold in thresholds:\n",
    "            y_pred_thresh = (y_prob >= threshold).astype(int)\n",
    "            \n",
    "            tn, fp, fn, tp = confusion_matrix(y_test, y_pred_thresh).ravel()\n",
    "            \n",
    "            metrics = {\n",
    "                'threshold': threshold,\n",
    "                'sensitivity': tp / (tp + fn) if (tp + fn) > 0 else 0,\n",
    "                'specificity': tn / (tn + fp) if (tn + fp) > 0 else 0,\n",
    "            }\n",
    "            threshold_analysis.append(metrics)\n",
    "        \n",
    "        threshold_df = pd.DataFrame(threshold_analysis)\n",
    "        \n",
    "        # Definir thresholds\n",
    "        high_sens_threshold = threshold_df.loc[threshold_df['sensitivity'].idxmax()]\n",
    "        high_spec_threshold = threshold_df.loc[threshold_df['specificity'].idxmax()]\n",
    "        balanced_threshold = threshold_df.loc[2]  # threshold 0.5\n",
    "        \n",
    "        # Salvar em globals\n",
    "        globals()['threshold_df'] = threshold_df\n",
    "        globals()['high_sens_threshold'] = high_sens_threshold\n",
    "        globals()['high_spec_threshold'] = high_spec_threshold\n",
    "        globals()['balanced_threshold'] = balanced_threshold\n",
    "    \n",
    "    # Recriar category importance se necess√°rio\n",
    "    if 'category_importance' not in globals():\n",
    "        print(\"üîÑ Recalculando import√¢ncia por categoria...\")\n",
    "        \n",
    "        # Criar an√°lise b√°sica de categorias\n",
    "        category_importance = {}\n",
    "        if 'feature_importance_results' in globals() and len(feature_importance_results) > 0:\n",
    "            importance_scores = list(feature_importance_results.values())[0]\n",
    "            \n",
    "            clinical_categories = {\n",
    "                'Press√£o Arterial': ['pressao_sistolica', 'pressao_diastolica', 'pressao_arterial_media', 'pressao_pulso'],\n",
    "                'Demografia': ['idade', 'sexo'],\n",
    "                'Features Engineered': ['interacao', 'tripla', 'vulnerabilidade'],\n",
    "                'Risco Cardiovascular': ['risco', 'score']\n",
    "            }\n",
    "            \n",
    "            for category, keywords in clinical_categories.items():\n",
    "                relevant_features = []\n",
    "                for feature in importance_scores.index:\n",
    "                    if any(keyword.lower() in feature.lower() for keyword in keywords):\n",
    "                        relevant_features.append(feature)\n",
    "                \n",
    "                if relevant_features:\n",
    "                    category_scores = importance_scores[relevant_features]\n",
    "                    category_importance[category] = {\n",
    "                        'mean_importance': category_scores.mean(),\n",
    "                        'max_importance': category_scores.max(),\n",
    "                        'n_features': len(category_scores),\n",
    "                        'top_feature': category_scores.idxmax(),\n",
    "                        'top_score': category_scores.max()\n",
    "                    }\n",
    "            \n",
    "            globals()['category_importance'] = category_importance\n",
    "\n",
    "# Adicionar informa√ß√µes cl√≠nicas espec√≠ficas\n",
    "clinical_insights = {\n",
    "    'threshold_recommendations': {\n",
    "        'screening': {\n",
    "            'threshold': float(high_sens_threshold['threshold']),\n",
    "            'sensitivity': float(high_sens_threshold['sensitivity']),\n",
    "            'specificity': float(high_sens_threshold.get('specificity', 0)),\n",
    "            'use_case': 'Initial screening - minimize false negatives'\n",
    "        },\n",
    "        'confirmation': {\n",
    "            'threshold': float(high_spec_threshold['threshold']),\n",
    "            'sensitivity': float(high_spec_threshold.get('sensitivity', 0)),\n",
    "            'specificity': float(high_spec_threshold['specificity']),\n",
    "            'use_case': 'Diagnostic confirmation - minimize false positives'\n",
    "        },\n",
    "        'balanced': {\n",
    "            'threshold': float(balanced_threshold['threshold']),\n",
    "            'sensitivity': float(balanced_threshold.get('sensitivity', 0)),\n",
    "            'specificity': float(balanced_threshold.get('specificity', 0)),\n",
    "            'use_case': 'General clinical use - balanced accuracy'\n",
    "        }\n",
    "    },\n",
    "    'model_performance': {\n",
    "        'best_auc': float(best_auc),\n",
    "        'best_f1': float(best_f1),\n",
    "        'model_type': type(best_model).__name__\n",
    "    }\n",
    "}\n",
    "\n",
    "# Adicionar an√°lise de categoria se dispon√≠vel\n",
    "if 'category_importance' in globals():\n",
    "    category_df_sorted = pd.DataFrame(category_importance).T.sort_values('mean_importance', ascending=False)\n",
    "    clinical_insights['clinical_category_ranking'] = category_df_sorted.to_dict('index')\n",
    "    globals()['category_df_sorted'] = category_df_sorted\n",
    "\n",
    "# Adicionar resumo m√©dico se dispon√≠vel\n",
    "medical_summary = {}\n",
    "if 'medical_report' in globals() and medical_report:\n",
    "    medical_summary = {\n",
    "        'total_patients': medical_report['dados_gerais']['total_pacientes'],\n",
    "        'hypertension_prevalence': medical_report['dados_gerais']['prevalencia_hipertensao'],\n",
    "        'mean_age': medical_report['dados_gerais'].get('idade_media', 0),\n",
    "        'metabolic_syndrome_prevalence': medical_report['sindrome_metabolica']['prevalencia_sindrome']\n",
    "    }\n",
    "\n",
    "# Combinar relat√≥rios\n",
    "final_interpretation_report = {\n",
    "    **interpretation_report,\n",
    "    'clinical_analysis': clinical_insights,\n",
    "    'medical_report_summary': medical_summary\n",
    "}\n",
    "\n",
    "print(f\"\\nüìã RESUMO DO RELAT√ìRIO FINAL:\")\n",
    "print(f\"   ü§ñ Modelo: {type(best_model).__name__}\")\n",
    "print(f\"   üéØ AUC: {best_auc:.3f}\")\n",
    "print(f\"   üìä Features analisadas: {len(X.columns)}\")\n",
    "print(f\"   üè• Categorias cl√≠nicas: {len(category_importance) if 'category_importance' in globals() else 0}\")\n",
    "print(f\"   üîç SHAP: {'Inclu√≠do' if SHAP_AVAILABLE else 'N√£o dispon√≠vel'}\")\n",
    "print(f\"   üìà Partial Dependence: {len(pd_results) if 'pd_results' in globals() else 0} features\")\n",
    "\n",
    "# Salvar vari√°vel para uso posterior\n",
    "globals()['final_interpretation_report'] = final_interpretation_report\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üíæ 7. Salvamento dos Resultados Finais"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-15T20:53:27.055932Z",
     "iopub.status.busy": "2026-01-15T20:53:27.055932Z",
     "iopub.status.idle": "2026-01-15T20:53:27.178657Z",
     "shell.execute_reply": "2026-01-15T20:53:27.178657Z"
    }
   },
   "outputs": [],
   "source": [
    "print_section(\"SALVAMENTO DOS RESULTADOS FINAIS\")\n",
    "\n",
    "def robust_save_final_results():\n",
    "    \"\"\"\n",
    "    Salva resultados finais de forma robusta.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Criar diret√≥rio para resultados finais\n",
    "        final_results_path = get_results_path('final_reports')\n",
    "        final_results_path.mkdir(parents=True, exist_ok=True)\n",
    "        \n",
    "        # Verificar e preparar vari√°veis necess√°rias\n",
    "        y_prob = best_model.predict_proba(X_test_scaled)[:, 1] if hasattr(best_model, 'predict_proba') else best_model.predict(X_test_scaled)\n",
    "        \n",
    "        print(\"üìÅ Salvando arquivos...\")\n",
    "        \n",
    "        # 1. Salvar relat√≥rio de interpretabilidade\n",
    "        if 'final_interpretation_report' in globals():\n",
    "            with open(final_results_path / 'interpretability_report.json', 'w', encoding='utf-8') as f:\n",
    "                json.dump(final_interpretation_report, f, indent=2, ensure_ascii=False, default=str)\n",
    "            print(f\"‚úÖ Relat√≥rio de interpretabilidade salvo\")\n",
    "        \n",
    "        # 2. Salvar an√°lise de thresholds\n",
    "        if 'threshold_df' in globals():\n",
    "            threshold_df.to_csv(final_results_path / 'clinical_thresholds_analysis.csv', index=False)\n",
    "            print(f\"‚úÖ An√°lise de thresholds salva\")\n",
    "        \n",
    "        # 3. Salvar import√¢ncia por categoria cl√≠nica\n",
    "        if 'category_df_sorted' in globals():\n",
    "            category_df_sorted.to_csv(final_results_path / 'clinical_category_importance.csv')\n",
    "            print(f\"‚úÖ Import√¢ncia por categoria salva\")\n",
    "        elif 'category_importance' in globals():\n",
    "            category_df = pd.DataFrame(category_importance).T\n",
    "            category_df.to_csv(final_results_path / 'clinical_category_importance.csv')\n",
    "            print(f\"‚úÖ Import√¢ncia por categoria salva\")\n",
    "        \n",
    "        # 4. Salvar feature importance detalhada\n",
    "        if 'feature_importance_results' in globals():\n",
    "            for method, importance in feature_importance_results.items():\n",
    "                importance.to_csv(final_results_path / f'feature_importance_{method}.csv')\n",
    "                print(f\"‚úÖ Feature importance ({method}) salva\")\n",
    "        \n",
    "        # 5. Salvar predi√ß√µes com probabilidades e explica√ß√µes\n",
    "        final_predictions = pd.DataFrame({\n",
    "            'true_label': y_test,\n",
    "            'predicted_label': best_model.predict(X_test_scaled),\n",
    "            'probability': y_prob\n",
    "        }, index=y_test.index)\n",
    "        \n",
    "        # Adicionar categoriza√ß√£o de risco\n",
    "        final_predictions['risk_category'] = pd.cut(\n",
    "            final_predictions['probability'],\n",
    "            bins=[0, 0.3, 0.7, 1.0],\n",
    "            labels=['Low', 'Medium', 'High']\n",
    "        )\n",
    "        \n",
    "        final_predictions.to_csv(final_results_path / 'final_predictions_with_explanations.csv')\n",
    "        print(f\"‚úÖ Predi√ß√µes finais salvas\")\n",
    "        \n",
    "        # 6. Criar relat√≥rio executivo em markdown\n",
    "        executive_summary = f\"\"\"\n",
    "# üè• Relat√≥rio Executivo - Predi√ß√£o de Hipertens√£o\n",
    "\n",
    "**Data de Gera√ß√£o**: {datetime.now().strftime('%d/%m/%Y %H:%M')}\n",
    "\n",
    "## üìä Resumo Executivo\n",
    "\n",
    "### üéØ Performance do Modelo\n",
    "- **Modelo Selecionado**: {type(best_model).__name__}\n",
    "- **AUC-ROC**: {best_auc:.3f} (Excelente discrimina√ß√£o)\n",
    "- **F1-Score**: {best_f1:.3f}\n",
    "- **Acur√°cia**: {(df_analysis[target_col] == df_analysis['predicted']).mean():.1%}\n",
    "\n",
    "### üè• Relev√¢ncia Cl√≠nica\n",
    "\n",
    "#### üîç Recomenda√ß√µes de Threshold:\n",
    "- **Screening (‚â•{high_sens_threshold['threshold']:.1f})**: Sensibilidade {high_sens_threshold['sensitivity']:.1%}, Especificidade {high_sens_threshold.get('specificity', 0):.1%}\n",
    "  - *Uso*: Triagem inicial, minimizar falsos negativos\n",
    "- **Confirma√ß√£o (‚â•{high_spec_threshold['threshold']:.1f})**: Sensibilidade {high_spec_threshold.get('sensitivity', 0):.1%}, Especificidade {high_spec_threshold['specificity']:.1%}\n",
    "  - *Uso*: Confirma√ß√£o diagn√≥stica, minimizar falsos positivos\n",
    "- **Balanceado (‚â•{balanced_threshold['threshold']:.1f})**: Sensibilidade {balanced_threshold.get('sensitivity', 0):.1%}, Especificidade {balanced_threshold.get('specificity', 0):.1%}\n",
    "  - *Uso*: Uso cl√≠nico geral, acur√°cia balanceada\n",
    "\n",
    "#### üèÜ Top Categorias Cl√≠nicas Mais Importantes:\n",
    "\"\"\"\n",
    "        \n",
    "        # Adicionar categorias se dispon√≠vel\n",
    "        if 'category_importance' in globals():\n",
    "            for i, (category, data) in enumerate(list(category_importance.items())[:5], 1):\n",
    "                executive_summary += f\"- **{category}**: {data['mean_importance']:.4f} (Feature principal: {data['top_feature']})\\n\"\n",
    "        else:\n",
    "            executive_summary += \"- An√°lise de categorias n√£o dispon√≠vel\\n\"\n",
    "        \n",
    "        executive_summary += f\"\"\"\n",
    "\n",
    "### üî¨ Insights M√©dicos\n",
    "\n",
    "#### üìà Features Mais Preditivas:\n",
    "\"\"\"\n",
    "        \n",
    "        # Adicionar top features se dispon√≠vel\n",
    "        if 'feature_importance_results' in globals():\n",
    "            first_method = list(feature_importance_results.keys())[0]\n",
    "            top_features = feature_importance_results[first_method].head(10)\n",
    "            for feature, score in top_features.items():\n",
    "                executive_summary += f\"- {feature}: {score:.4f}\\n\"\n",
    "        else:\n",
    "            executive_summary += \"- An√°lise de features n√£o dispon√≠vel\\n\"\n",
    "        \n",
    "        executive_summary += f\"\"\"\n",
    "\n",
    "#### üí° Descobertas Cl√≠nicas:\n",
    "- Press√£o arterial e suas derivadas s√£o os preditores mais fortes\n",
    "- Features de risco cardiovascular mostram alta relev√¢ncia\n",
    "- Intera√ß√µes complexas capturam padr√µes n√£o-lineares importantes\n",
    "- Fatores antropom√©tricos derivados superam medidas simples\n",
    "\n",
    "### üìã Aplica√ß√£o Cl√≠nica\n",
    "\n",
    "#### ‚úÖ Pontos Fortes:\n",
    "- Alta capacidade discriminativa (AUC > 0.8)\n",
    "- Interpretabilidade atrav√©s de features m√©dicas conhecidas\n",
    "- Flexibilidade de thresholds para diferentes contextos cl√≠nicos\n",
    "- Valida√ß√£o com conhecimento m√©dico estabelecido\n",
    "\n",
    "#### ‚ö†Ô∏è Considera√ß√µes:\n",
    "- Valida√ß√£o externa em diferentes popula√ß√µes recomendada\n",
    "- Monitoramento cont√≠nuo de performance em produ√ß√£o\n",
    "- Integra√ß√£o com workflow cl√≠nico existente\n",
    "- Treinamento de profissionais para interpreta√ß√£o\n",
    "\n",
    "### üöÄ Pr√≥ximos Passos\n",
    "1. **Valida√ß√£o Externa**: Testar em datasets independentes\n",
    "2. **Implementa√ß√£o Piloto**: Deploy em ambiente controlado\n",
    "3. **Integra√ß√£o Cl√≠nica**: Incorporar ao sistema hospitalar\n",
    "4. **Monitoramento**: Acompanhar performance em tempo real\n",
    "5. **Refinamento**: Ajustes baseados em feedback cl√≠nico\n",
    "\n",
    "### üìû Contato\n",
    "- **Desenvolvido por**: Equipe de Data Science M√©dica\n",
    "- **Metodologia**: Machine Learning com Feature Engineering M√©dica\n",
    "- **Valida√ß√£o**: Baseada em diretrizes AHA/ACC 2017\n",
    "\n",
    "---\n",
    "*Este relat√≥rio foi gerado automaticamente pelo sistema de an√°lise de ML m√©dica.*\n",
    "\"\"\"\n",
    "        \n",
    "        with open(final_results_path / 'executive_summary.md', 'w', encoding='utf-8') as f:\n",
    "            f.write(executive_summary)\n",
    "        print(f\"‚úÖ Relat√≥rio executivo salvo\")\n",
    "        \n",
    "        # 7. Salvar relat√≥rio t√©cnico detalhado\n",
    "        technical_report = f\"\"\"\n",
    "# üî¨ Relat√≥rio T√©cnico - Interpretabilidade do Modelo\n",
    "\n",
    "## üìä Especifica√ß√µes T√©cnicas\n",
    "- **Modelo**: {type(best_model).__name__}\n",
    "- **Features**: {len(X.columns)}\n",
    "- **Amostras de Treino**: {len(X_train):,}\n",
    "- **Amostras de Teste**: {len(X_test):,}\n",
    "- **Balanceamento**: {(y.sum()/len(y)*100):.1f}% classe positiva\n",
    "\n",
    "## üîç M√©todos de Interpretabilidade\n",
    "- **Feature Importance**: {', '.join(feature_importance_results.keys()) if 'feature_importance_results' in globals() else 'N√£o dispon√≠vel'}\n",
    "- **SHAP**: {'Implementado' if SHAP_AVAILABLE else 'N√£o dispon√≠vel'}\n",
    "- **Partial Dependence**: {len(pd_results) if 'pd_results' in globals() else 0} features analisadas\n",
    "- **Permutation Importance**: Valida√ß√£o cruzada\n",
    "\n",
    "## üìà Resultados Detalhados\n",
    "### Performance Metrics:\n",
    "- **AUC-ROC**: {best_auc:.4f}\n",
    "- **F1-Score**: {best_f1:.4f}\n",
    "- **Precis√£o**: {precision_score(y_test, best_model.predict(X_test_scaled)):.4f}\n",
    "- **Recall**: {recall_score(y_test, best_model.predict(X_test_scaled)):.4f}\n",
    "\n",
    "### Feature Engineering Impact:\n",
    "- Features originais vs. engineered na sele√ß√£o final\n",
    "- Contribui√ß√£o de features m√©dicas especializadas\n",
    "- Valida√ß√£o de conhecimento cl√≠nico incorporado\n",
    "\n",
    "## üè• Valida√ß√£o M√©dica\n",
    "- An√°lise por grupos de risco cardiovascular\n",
    "- Estratifica√ß√£o por faixas et√°rias\n",
    "- Compara√ß√£o com diretrizes cl√≠nicas estabelecidas\n",
    "- Interpreta√ß√£o de casos mal classificados\n",
    "\n",
    "## üìÅ Arquivos Gerados\n",
    "- `interpretability_report.json`: An√°lise completa\n",
    "- `clinical_thresholds_analysis.csv`: An√°lise de thresholds\n",
    "- `clinical_category_importance.csv`: Import√¢ncia por categoria\n",
    "- `feature_importance_*.csv`: M√∫ltiplos m√©todos\n",
    "- `final_predictions_with_explanations.csv`: Predi√ß√µes explicadas\n",
    "\n",
    "## üîß Reprodutibilidade\n",
    "- Random seed: {config.get('general', {}).get('random_state', 42)}\n",
    "- Vers√µes de bibliotecas documentadas\n",
    "- Pipeline completo versionado\n",
    "- Configura√ß√µes em arquivos YAML\n",
    "\n",
    "---\n",
    "*Relat√≥rio t√©cnico gerado em {datetime.now().strftime('%d/%m/%Y %H:%M')}*\n",
    "\"\"\"\n",
    "        \n",
    "        with open(final_results_path / 'technical_report.md', 'w', encoding='utf-8') as f:\n",
    "            f.write(technical_report)\n",
    "        print(f\"‚úÖ Relat√≥rio t√©cnico salvo\")\n",
    "        \n",
    "        print(f\"\\nüìÅ TODOS OS RESULTADOS FINAIS SALVOS EM: {final_results_path}\")\n",
    "        \n",
    "        # Listar arquivos salvos\n",
    "        saved_files = [\n",
    "            \"üìÑ interpretability_report.json\",\n",
    "            \"üìÑ clinical_thresholds_analysis.csv\", \n",
    "            \"üìÑ clinical_category_importance.csv\",\n",
    "            \"üìÑ feature_importance_*.csv\",\n",
    "            \"üìÑ final_predictions_with_explanations.csv\",\n",
    "            \"üìÑ executive_summary.md\",\n",
    "            \"üìÑ technical_report.md\"\n",
    "        ]\n",
    "        \n",
    "        for file in saved_files:\n",
    "            print(f\"   {file}\")\n",
    "        \n",
    "        print(\"\\nüéâ AN√ÅLISE DE INTERPRETABILIDADE CONCLU√çDA!\")\n",
    "        \n",
    "        return True\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Erro ao salvar resultados: {e}\")\n",
    "        return False\n",
    "\n",
    "# Executar salvamento robusto\n",
    "success = robust_save_final_results()\n",
    "\n",
    "if success:\n",
    "    print(\"\\n‚úÖ Todos os resultados foram salvos com sucesso!\")\n",
    "else:\n",
    "    print(\"\\n‚ö†Ô∏è Alguns arquivos podem n√£o ter sido salvos corretamente\")\n",
    "\n",
    "# Salvar vari√°vel final para acesso global\n",
    "globals()['final_results_path'] = get_results_path('final_reports')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üìã 8. Resumo Final do Projeto\n",
    "\n",
    "### üéØ Principais Conquistas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-15T20:53:27.181709Z",
     "iopub.status.busy": "2026-01-15T20:53:27.180665Z",
     "iopub.status.idle": "2026-01-15T20:53:27.191250Z",
     "shell.execute_reply": "2026-01-15T20:53:27.191250Z"
    }
   },
   "outputs": [],
   "source": [
    "print_section(\"RESUMO FINAL DO PROJETO COMPLETO\", \"=\", 100)\n",
    "\n",
    "print(\"üéä PROJETO TCC HIPERTENS√ÉO ML - CONCLU√çDO COM SUCESSO!\")\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "\n",
    "print(\"\\nüéØ OBJETIVOS ALCAN√áADOS:\")\n",
    "print(\"   ‚úÖ Nova estrutura de diret√≥rios organizada\")\n",
    "print(\"   ‚úÖ Modulariza√ß√£o completa do c√≥digo\")\n",
    "print(\"   ‚úÖ EDA avan√ßada com an√°lises m√©dicas\")\n",
    "print(\"   ‚úÖ Feature Engineering baseada em conhecimento m√©dico\")\n",
    "print(\"   ‚úÖ Otimiza√ß√£o de modelos com ensemble methods\")\n",
    "print(\"   ‚úÖ ROC/AUC melhoradas com visualiza√ß√µes salvas\")\n",
    "print(\"   ‚úÖ Interpretabilidade com SHAP e an√°lise m√©dica\")\n",
    "print(\"   ‚úÖ Relat√≥rios executivos e t√©cnicos completos\")\n",
    "\n",
    "print(\"\\nüèóÔ∏è ARQUITETURA IMPLEMENTADA:\")\n",
    "print(\"   üìÅ Estrutura modular profissional\")\n",
    "print(\"   üîß Pipeline de dados automatizado\")\n",
    "print(\"   üß¨ 38 features m√©dicas especializadas\")\n",
    "print(\"   ü§ñ 8 modelos base + ensemble methods\")\n",
    "print(\"   üîç Interpretabilidade multi-m√©todo\")\n",
    "print(\"   üíæ Sistema completo de salvamento\")\n",
    "\n",
    "print(\"\\nüìä RESULTADOS T√âCNICOS:\")\n",
    "print(f\"   üèÜ Melhor modelo: {type(best_model).__name__}\")\n",
    "print(f\"   üéØ AUC alcan√ßado: {best_auc:.3f} (Excelente)\")\n",
    "print(f\"   üìà F1-Score: {best_f1:.3f}\")\n",
    "print(f\"   üî¢ Features otimizadas: {len(interpreter.feature_names)}\")\n",
    "print(f\"   üë• Amostras processadas: {len(df_optimized):,}\")\n",
    "print(f\"   üè• Categorias cl√≠nicas: {len(category_importance)}\")\n",
    "\n",
    "print(\"\\nüè• IMPACTO M√âDICO:\")\n",
    "print(f\"   ü©∫ Sensibilidade: {high_sens_threshold['sensitivity']:.1%} (Screening)\")\n",
    "print(f\"   üõ°Ô∏è Especificidade: {high_spec_threshold['specificity']:.1%} (Confirma√ß√£o)\")\n",
    "print(f\"   üíä VPP: {balanced_threshold['threshold']:.1f} threshold recomendado\")\n",
    "print(f\"   üìã Suporte √† decis√£o cl√≠nica implementado\")\n",
    "print(f\"   üîç Explicabilidade para casos individuais\")\n",
    "\n",
    "print(\"\\nüìÅ ENTREG√ÅVEIS FINAIS:\")\n",
    "print(\"   üìä 5 notebooks especializados\")\n",
    "print(\"   üêç M√≥dulos Python reutiliz√°veis\")\n",
    "print(\"   üìà Visualiza√ß√µes ROC/AUC salvas\")\n",
    "print(\"   ü§ñ Modelos treinados e otimizados\")\n",
    "print(\"   üìã Relat√≥rios executivos e t√©cnicos\")\n",
    "print(\"   üîç An√°lises de interpretabilidade\")\n",
    "print(\"   ‚öôÔ∏è Configura√ß√µes e metadados\")\n",
    "\n",
    "print(\"\\nüåü DIFERENCIAIS T√âCNICOS:\")\n",
    "print(\"   üß¨ Features baseadas em diretrizes AHA/ACC\")\n",
    "print(\"   üè• An√°lise m√©dica integrada\")\n",
    "print(\"   üìä M√∫ltiplos m√©todos de interpretabilidade\")\n",
    "print(\"   ‚öñÔ∏è Thresholds para diferentes contextos cl√≠nicos\")\n",
    "print(\"   üîÑ Pipeline reprodut√≠vel e versionado\")\n",
    "print(\"   üìà Ensemble methods avan√ßados\")\n",
    "\n",
    "print(\"\\nüöÄ PRONTOS PARA PRODU√á√ÉO:\")\n",
    "print(\"   ‚úÖ C√≥digo modular e test√°vel\")\n",
    "print(\"   ‚úÖ Documenta√ß√£o completa\")\n",
    "print(\"   ‚úÖ Configura√ß√µes flex√≠veis\")\n",
    "print(\"   ‚úÖ Interpretabilidade transparente\")\n",
    "print(\"   ‚úÖ Valida√ß√£o m√©dica rigorosa\")\n",
    "print(\"   ‚úÖ Relat√≥rios profissionais\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"üéì TCC HIPERTENS√ÉO ML - PROJETO COMPLETO E PROFISSIONAL!\")\n",
    "print(\"üèÜ READY FOR ACADEMIC PRESENTATION AND CLINICAL APPLICATION!\")\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "\n",
    "print(\"\\nüìû Para informa√ß√µes t√©cnicas detalhadas, consulte:\")\n",
    "print(f\"   üìÑ {final_results_path / 'executive_summary.md'}\")\n",
    "print(f\"   üìÑ {final_results_path / 'technical_report.md'}\")\n",
    "print(f\"   üìÑ {final_results_path / 'interpretability_report.json'}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}