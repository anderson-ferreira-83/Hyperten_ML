#!/usr/bin/env python3
"""
Teste da Valida√ß√£o Cl√≠nica - Vers√£o Simplificada
Demonstra o sistema de valida√ß√£o implementado
"""

import pandas as pd
import numpy as np
from datetime import datetime
from pathlib import Path
import json

def simulate_clinical_validation():
    """Simular valida√ß√£o cl√≠nica com dados dispon√≠veis"""
    
    print("üöÄ TESTE DE VALIDA√á√ÉO CL√çNICA AUTOMATIZADA")
    print("Baseado na metodologia do projeto A1_A2")
    print("="*80)
    
    # Simular carregamento de dados
    print("üìÅ Simulando carregamento de modelo e dados...")
    
    # Usar dados dispon√≠veis
    data_path = Path('results/data/feature_engineered_enhanced_selected.csv')
    if data_path.exists():
        df = pd.read_csv(data_path)
        print(f"‚úÖ Dados carregados: {df.shape}")
    else:
        print("‚ùå Dados n√£o encontrados, criando dados sint√©ticos para demonstra√ß√£o...")
        
        # Criar dados sint√©ticos para demonstra√ß√£o
        np.random.seed(42)
        n_samples = 1000
        
        df = pd.DataFrame({
            'score_risco_cv': np.random.normal(0.5, 0.2, n_samples),
            'pressao_sistolica': np.random.normal(130, 20, n_samples),
            'pressao_diastolica': np.random.normal(85, 10, n_samples),
            'pressao_arterial_media': np.random.normal(100, 15, n_samples),
            'idade': np.random.normal(50, 15, n_samples),
            'risco_hipertensao': np.random.binomial(1, 0.3, n_samples)
        })
        print(f"‚úÖ Dados sint√©ticos criados: {df.shape}")
    
    # Separar features e target
    target_col = 'risco_hipertensao'
    if target_col not in df.columns:
        target_col = df.columns[-1]
    
    X = df.drop(columns=[target_col])
    y = df[target_col]
    
    print(f"‚úÖ Features: {X.shape[1]}, Target: {target_col}")
    print(f"üìä Distribui√ß√£o target: {dict(y.value_counts())}")
    
    # Simular predi√ß√µes de modelo
    print("\nüìä Simulando predi√ß√µes do modelo...")
    
    # Simular probabilidades baseadas nas features (correla√ß√£o real√≠stica)
    y_proba = np.random.beta(2, 5, len(y))  # Distribui√ß√£o mais real√≠stica
    
    # Adicionar correla√ß√£o com features importantes
    if 'pressao_sistolica' in X.columns:
        # Correla√ß√£o positiva com press√£o sist√≥lica
        systolic_norm = (X['pressao_sistolica'] - X['pressao_sistolica'].min()) / (X['pressao_sistolica'].max() - X['pressao_sistolica'].min())
        y_proba = 0.7 * y_proba + 0.3 * systolic_norm
    
    y_proba = np.clip(y_proba, 0.01, 0.99)  # Limitar entre 0.01 e 0.99
    y_pred = (y_proba > 0.5).astype(int)
    
    print("‚úÖ Predi√ß√µes simuladas geradas")
    
    # Executar valida√ß√µes
    results = run_validation_demo(X, y, y_pred, y_proba, df)
    
    # Salvar resultados
    save_demo_results(results)
    
    print(f"\n‚úÖ DEMONSTRA√á√ÉO DE VALIDA√á√ÉO CL√çNICA CONCLU√çDA!")
    return results

def run_validation_demo(X, y, y_pred, y_proba, df):
    """Executar demonstra√ß√£o das valida√ß√µes"""
    
    results = {
        'timestamp': datetime.now().isoformat(),
        'demo_mode': True,
        'validations': {}
    }
    
    # 1. Valida√ß√£o M√©dica B√°sica
    print(f"\nüîç 1. VALIDA√á√ÉO CONTRA CONHECIMENTO M√âDICO")
    print("-" * 50)
    
    medical_validation = validate_medical_logic(X, y, y_pred, y_proba)
    results['validations']['medical'] = medical_validation
    
    print(f"üìä Score de consist√™ncia m√©dica: {medical_validation['consistency_score']:.3f}")
    print(f"üìã {medical_validation['interpretation']}")
    
    # 2. An√°lise de Thresholds
    print(f"\n‚öñÔ∏è 2. AN√ÅLISE DE THRESHOLDS CL√çNICOS")
    print("-" * 50)
    
    threshold_analysis = analyze_thresholds(y, y_proba)
    results['validations']['thresholds'] = threshold_analysis
    
    print(f"üìä THRESHOLDS ANALISADOS:")
    for scenario, metrics in threshold_analysis['scenarios'].items():
        print(f"   {scenario}: {metrics['threshold']:.3f} "
              f"(Sens: {metrics['sensitivity']:.1%}, Spec: {metrics['specificity']:.1%})")
    
    # 3. An√°lise de Propor√ß√µes
    print(f"\nüìä 3. AN√ÅLISE DE PROPOR√á√ïES")
    print("-" * 50)
    
    proportion_analysis = analyze_proportions(y, y_proba)
    results['validations']['proportions'] = proportion_analysis
    
    print(f"üìä CEN√ÅRIOS ANALISADOS:")
    for scenario, config in proportion_analysis['scenarios'].items():
        print(f"   {scenario}: {config['target_prevalence']:.1%} "
              f"(Performance estimada: {config['estimated_performance']:.3f})")
    
    return results

def validate_medical_logic(X, y, y_pred, y_proba):
    """Valida√ß√£o b√°sica da l√≥gica m√©dica"""
    
    validation = {
        'features_analyzed': list(X.columns),
        'correlations': {},
        'consistency_score': 0,
        'interpretation': ''
    }
    
    # Analisar correla√ß√µes com features m√©dicas importantes
    medical_features = {
        'pressao_sistolica': 'Press√£o Sist√≥lica',
        'pressao_diastolica': 'Press√£o Diast√≥lica', 
        'pressao_arterial_media': 'Press√£o Arterial M√©dia',
        'idade': 'Idade',
        'score_risco': 'Score de Risco'
    }
    
    correlations_found = 0
    total_correlations = 0
    
    for feature_key, feature_name in medical_features.items():
        # Buscar features que contenham a palavra-chave
        matching_features = [f for f in X.columns if feature_key.lower() in f.lower()]
        
        if matching_features:
            feature = matching_features[0]  # Usar primeira correspond√™ncia
            
            # Calcular correla√ß√£o com predi√ß√µes
            try:
                correlation = np.corrcoef(X[feature], y_proba)[0, 1]
                
                validation['correlations'][feature_name] = {
                    'feature_used': feature,
                    'correlation': correlation,
                    'expected_positive': True,  # Esperamos correla√ß√£o positiva
                    'meets_expectation': correlation > 0.1
                }
                
                if correlation > 0.1:
                    correlations_found += 1
                total_correlations += 1
                
            except Exception as e:
                validation['correlations'][feature_name] = {
                    'error': str(e)
                }
    
    # Calcular score de consist√™ncia
    if total_correlations > 0:
        consistency_score = correlations_found / total_correlations
    else:
        consistency_score = 0
    
    validation['consistency_score'] = consistency_score
    
    # Interpreta√ß√£o
    if consistency_score >= 0.7:
        validation['interpretation'] = "Excelente consist√™ncia com conhecimento m√©dico"
    elif consistency_score >= 0.5:
        validation['interpretation'] = "Boa consist√™ncia m√©dica"
    elif consistency_score >= 0.3:
        validation['interpretation'] = "Consist√™ncia m√©dica moderada"
    else:
        validation['interpretation'] = "Baixa consist√™ncia m√©dica - revis√£o recomendada"
    
    return validation

def analyze_thresholds(y_true, y_proba):
    """An√°lise b√°sica de thresholds"""
    
    from sklearn.metrics import confusion_matrix
    
    scenarios = {
        'screening': {
            'description': 'Triagem - Alta Sensibilidade',
            'threshold': 0.3
        },
        'balanced': {
            'description': 'Diagn√≥stico Balanceado',
            'threshold': 0.5
        },
        'confirmation': {
            'description': 'Confirma√ß√£o - Alta Especificidade',
            'threshold': 0.7
        }
    }
    
    threshold_analysis = {
        'scenarios': {}
    }
    
    for scenario_name, scenario_config in scenarios.items():
        threshold = scenario_config['threshold']
        y_pred_thresh = (y_proba >= threshold).astype(int)
        
        # Calcular m√©tricas
        try:
            tn, fp, fn, tp = confusion_matrix(y_true, y_pred_thresh).ravel()
            
            sensitivity = tp / (tp + fn) if (tp + fn) > 0 else 0
            specificity = tn / (tn + fp) if (tn + fp) > 0 else 0
            accuracy = (tp + tn) / (tp + tn + fp + fn) if (tp + tn + fp + fn) > 0 else 0
            
            threshold_analysis['scenarios'][scenario_name] = {
                'threshold': threshold,
                'description': scenario_config['description'],
                'sensitivity': sensitivity,
                'specificity': specificity,
                'accuracy': accuracy,
                'confusion_matrix': {'tp': int(tp), 'fp': int(fp), 'tn': int(tn), 'fn': int(fn)}
            }
            
        except Exception as e:
            threshold_analysis['scenarios'][scenario_name] = {
                'threshold': threshold,
                'error': str(e)
            }
    
    return threshold_analysis

def analyze_proportions(y, y_proba):
    """An√°lise b√°sica de propor√ß√µes"""
    
    current_prevalence = y.mean()
    
    scenarios = {
        'screening': {
            'target_prevalence': 0.05,
            'description': 'Cen√°rio de triagem populacional'
        },
        'general': {
            'target_prevalence': current_prevalence,
            'description': 'Popula√ß√£o geral (preval√™ncia atual)'
        },
        'high_risk': {
            'target_prevalence': 0.60,
            'description': 'Coorte de alto risco'
        }
    }
    
    proportion_analysis = {
        'current_prevalence': current_prevalence,
        'scenarios': {}
    }
    
    for scenario_name, scenario_config in scenarios.items():
        # Simular performance baseada na diferen√ßa da preval√™ncia atual
        target_prev = scenario_config['target_prevalence']
        
        # Estimar performance (simula√ß√£o simplificada)
        prevalence_diff = abs(target_prev - current_prevalence)
        
        # Performance decresce com maior diferen√ßa da preval√™ncia atual
        estimated_performance = max(0.5, 1.0 - prevalence_diff * 2)
        
        proportion_analysis['scenarios'][scenario_name] = {
            'target_prevalence': target_prev,
            'description': scenario_config['description'],
            'prevalence_difference': prevalence_diff,
            'estimated_performance': estimated_performance,
            'recommendation': get_proportion_recommendation(prevalence_diff, estimated_performance)
        }
    
    return proportion_analysis

def get_proportion_recommendation(prevalence_diff, performance):
    """Gerar recomenda√ß√£o baseada na diferen√ßa de preval√™ncia"""
    
    if prevalence_diff < 0.1:
        return "Cen√°rio √≥timo - baixa adapta√ß√£o necess√°ria"
    elif prevalence_diff < 0.2:
        return "Cen√°rio bom - adapta√ß√£o moderada necess√°ria"
    elif prevalence_diff < 0.3:
        return "Cen√°rio aceit√°vel - adapta√ß√£o significativa necess√°ria"
    else:
        return "Cen√°rio desafiador - retreinamento pode ser necess√°rio"

def save_demo_results(results):
    """Salvar resultados da demonstra√ß√£o"""
    
    save_path = Path('3_CLINICAL_VALIDATION')
    save_path.mkdir(exist_ok=True)
    
    timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
    
    # Salvar resultados completos
    with open(save_path / f'clinical_validation_demo_{timestamp}.json', 'w', encoding='utf-8') as f:
        json.dump(results, f, indent=2, ensure_ascii=False, default=str)
    
    # Criar resumo
    summary = f"""
DEMONSTRA√á√ÉO DE VALIDA√á√ÉO CL√çNICA AUTOMATIZADA
=============================================

Data/Hora: {results['timestamp']}
Modo: Demonstra√ß√£o

RESULTADOS DA VALIDA√á√ÉO:
{'-'*40}

1. VALIDA√á√ÉO M√âDICA:
   Score de Consist√™ncia: {results['validations']['medical']['consistency_score']:.3f}
   Interpreta√ß√£o: {results['validations']['medical']['interpretation']}

2. AN√ÅLISE DE THRESHOLDS:
"""
    
    for scenario, metrics in results['validations']['thresholds']['scenarios'].items():
        if 'error' not in metrics:
            summary += f"""   {scenario.upper()}:
     Threshold: {metrics['threshold']:.3f}
     Sensibilidade: {metrics['sensitivity']:.1%}
     Especificidade: {metrics['specificity']:.1%}
     Acur√°cia: {metrics['accuracy']:.1%}

"""
    
    summary += "3. AN√ÅLISE DE PROPOR√á√ïES:\n"
    for scenario, config in results['validations']['proportions']['scenarios'].items():
        summary += f"""   {scenario.upper()}:
     Preval√™ncia Alvo: {config['target_prevalence']:.1%}
     Performance Estimada: {config['estimated_performance']:.3f}
     Recomenda√ß√£o: {config['recommendation']}

"""
    
    summary += f"""
CONCLUS√ÉO:
{'-'*40}
‚úÖ Sistema de valida√ß√£o cl√≠nica implementado com sucesso!
‚úÖ Estrutura baseada na metodologia do projeto A1_A2
‚úÖ Valida√ß√£o m√©dica, thresholds e propor√ß√µes analisados
‚úÖ Resultados salvos para an√°lise posterior

Arquivos gerados:
- clinical_validation_demo_{timestamp}.json
- clinical_validation_demo_summary_{timestamp}.txt

Para uso em produ√ß√£o, conecte com modelo real e execute:
python clinical_validation_runner.py
"""
    
    with open(save_path / f'clinical_validation_demo_summary_{timestamp}.txt', 'w', encoding='utf-8') as f:
        f.write(summary)
    
    print(f"\nüíæ Resultados da demonstra√ß√£o salvos em: {save_path}")
    print(f"   üìÑ clinical_validation_demo_{timestamp}.json")
    print(f"   üìÑ clinical_validation_demo_summary_{timestamp}.txt")

if __name__ == "__main__":
    results = simulate_clinical_validation()